{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sending a POST Request to a Local LLM API**\n",
        "This block demonstrates how to send a JSON-formatted request to a locally hosted Language Model API using requests.\n",
        "\n",
        "**BASE_URL:** The root address for the API ‚Äî usually exposed via ngrok when hosting locally.\n",
        "\n",
        "**ENDPOINT:** Specific path where the model API listens for requests (e.g., /api/generate).\n",
        "\n",
        "**url:** Full URL used for the POST request.\n",
        "\n",
        "\n",
        "**model:** Specifies the name/version of the language model to use.\n",
        "\n",
        "**prompt:** The input query for the model to respond to.\n",
        "\n",
        "**\"Content-Type\":** \"application/json\" indicates the request body format.\n",
        "\n",
        "The Host header is intentionally omitted (handled by ngrok).\n",
        "\n"
      ],
      "metadata": {
        "id": "HKgDQel1F7Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "url = BASE_URL + ENDPOINT\n",
        "\n",
        "payload = {\"model\": \"llama3.2\", \"prompt\": \"Hello, world!\"}\n",
        "headers = {\"Content-Type\": \"application/json\"}  # no Host header here\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "print(response.status_code)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNvyNyHMJqP1",
        "outputId": "14bf22ae-a872-49f8-f1fb-7b08628b181e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:11.93036Z\",\"response\":\"Hello\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:11.963882Z\",\"response\":\"!\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.0152Z\",\"response\":\" It\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.059124Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.103789Z\",\"response\":\" nice\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.14873Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.191039Z\",\"response\":\" meet\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.233398Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.276143Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.318491Z\",\"response\":\" Is\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.364815Z\",\"response\":\" there\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.409124Z\",\"response\":\" something\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.451639Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.493522Z\",\"response\":\" can\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.536971Z\",\"response\":\" help\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.578949Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.624086Z\",\"response\":\" with\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.670237Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.711932Z\",\"response\":\" would\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.753394Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.795534Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.836716Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.877738Z\",\"response\":\" chat\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.920381Z\",\"response\":\"?\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:12.966189Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,9906,11,1917,0,128009,128006,78191,128007,271,9906,0,1102,596,6555,311,3449,499,13,2209,1070,2555,358,649,1520,499,449,477,1053,499,1093,311,6369,30],\"total_duration\":3636784584,\"load_duration\":836650125,\"prompt_eval_count\":29,\"prompt_eval_duration\":1748000000,\"eval_count\":25,\"eval_duration\":1048000000}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simulated KV Cache for LLM Inference Using Ollama**\n",
        "This section demonstrates how to simulate KV caching for large language model (LLM) inference, using token tracking and prompt reconstruction in a stateless API setup (like Ollama served over ngrok).\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "Simulate KV cache behavior by manually storing and reusing past tokens.\n",
        "\n",
        "Bypasses the limitation that Ollama does not support injecting past key-value pairs.\n",
        "\n",
        " **Features**\n",
        "\n",
        "**Tokenization:** Naively splits text using spaces.\n",
        "\n",
        "**Storage:** Maintains a rolling buffer (max_cache_size) of tokens.\n",
        "\n",
        "**Retrieval:** Converts token list back to a text prompt.\n",
        "\n",
        "**Reset:** Provides a method to clear the cache.\n",
        "\n",
        "**Example Methods**\n",
        "\n",
        "**add_tokens(new_text):** Appends new tokens and truncates to buffer limit.\n",
        "\n",
        "**get_cached_tokens():** Returns all stored tokens as a string.\n",
        "\n",
        "**clear_cache():** Resets the internal cache.\n",
        "\n"
      ],
      "metadata": {
        "id": "rttfPadHG5EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "\n",
        "class TokenCache:\n",
        "    \"\"\"\n",
        "    Simulates KV caching by storing past tokens and reusing them in future inferences.\n",
        "    Since Ollama does not allow KV cache injection, we track tokens manually.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_cache_size=1024):\n",
        "        self.max_cache_size = max_cache_size\n",
        "        self.tokens = []\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Tokenizes and stores the last max_cache_size tokens.\"\"\"\n",
        "        new_tokens = new_text.split()  # Simulate tokenization\n",
        "        self.tokens.extend(new_tokens)\n",
        "\n",
        "        # Limit the cache size\n",
        "        if len(self.tokens) > self.max_cache_size:\n",
        "            self.tokens = self.tokens[-self.max_cache_size:]\n",
        "\n",
        "    def get_cached_tokens(self):\n",
        "        \"\"\"Retrieves stored tokens as a single text input.\"\"\"\n",
        "        return \" \".join(self.tokens)  # Convert stored tokens into a single string\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token cache.\"\"\"\n",
        "        self.tokens = []\n",
        "        print(\"üóëÔ∏è Token cache cleared.\")\n",
        "\n",
        "# Initialize the token-based cache\n",
        "token_cache = TokenCache(max_cache_size=1024)\n",
        "\n",
        "def run_inference_with_kv_cache(api_url, model_name, new_input, token_cache):\n",
        "    \"\"\"\n",
        "    Runs inference using Ollama while simulating KV caching by keeping track of past tokens.\n",
        "    \"\"\"\n",
        "    # 1Ô∏è‚É£ Retrieve stored tokens\n",
        "    cached_context = token_cache.get_cached_tokens()\n",
        "\n",
        "    # 2Ô∏è‚É£ Create new input including stored tokens\n",
        "    full_prompt = cached_context + \" \" + new_input if cached_context else new_input\n",
        "\n",
        "    # 3Ô∏è‚É£ Send request to Ollama\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        print(\"\\nüì° Raw API Response:\")\n",
        "\n",
        "        full_response = \"\"\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    full_response += parsed_line.get(\"response\", \"\") + \" \"\n",
        "                    print(json_line)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing line: {line}, Error: {e}\")\n",
        "\n",
        "        # 4Ô∏è‚É£ Store generated tokens for future inference\n",
        "        token_cache.add_tokens(full_response.strip())\n",
        "\n",
        "        return {\"response\": full_response.strip()}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# üîπ Test the KV cache with Ollama\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Run multiple inferences\n",
        "response1 = run_inference_with_kv_cache(OLLAMA_API_URL, \"llama3.2\", \"Hello, how are you?\", token_cache)\n",
        "print(\"\\nResponse 1:\", response1)\n",
        "\n",
        "response2 = run_inference_with_kv_cache(OLLAMA_API_URL, \"llama3.2\", \"What is your name?\", token_cache)\n",
        "print(\"\\nResponse 2:\", response2)\n",
        "\n",
        "response3 = run_inference_with_kv_cache(OLLAMA_API_URL, \"llama3.2\", \"Tell me a joke.\", token_cache)\n",
        "print(\"\\nResponse 3:\", response3)\n",
        "\n",
        "# üóëÔ∏è Clear cache and test again\n",
        "token_cache.clear_cache()\n",
        "response4 = run_inference_with_kv_cache(OLLAMA_API_URL, \"llama3.2\", \"What do you remember?\", token_cache)\n",
        "print(\"\\nResponse 4:\", response4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbWe8mfai7yG",
        "outputId": "239afb59-e971-45cb-9381-23ffe18ceaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì° Raw API Response:\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.029406Z\",\"response\":\"I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.069654Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.112543Z\",\"response\":\" just\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.154186Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.198319Z\",\"response\":\" language\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.239625Z\",\"response\":\" model\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.281594Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.325858Z\",\"response\":\" so\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.371256Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.413066Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.455032Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.498263Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.54153Z\",\"response\":\" emotions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.582493Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.623768Z\",\"response\":\" feelings\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.669352Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.711905Z\",\"response\":\" humans\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.754042Z\",\"response\":\" do\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.794647Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.83744Z\",\"response\":\" However\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.880483Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.923591Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:54.969526Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.012445Z\",\"response\":\" functioning\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.055308Z\",\"response\":\" properly\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.096796Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.140531Z\",\"response\":\" ready\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.184709Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.226201Z\",\"response\":\" assist\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.268923Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.309988Z\",\"response\":\" with\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.351947Z\",\"response\":\" any\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.393103Z\",\"response\":\" questions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.43505Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.477669Z\",\"response\":\" tasks\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.530483Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.573486Z\",\"response\":\" may\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.616863Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.659535Z\",\"response\":\"!\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.700481Z\",\"response\":\" How\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.741571Z\",\"response\":\" can\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.783269Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.824683Z\",\"response\":\" help\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.866847Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.907807Z\",\"response\":\" today\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.954603Z\",\"response\":\"?\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:55.998799Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,9906,11,1268,527,499,30,128009,128006,78191,128007,271,40,2846,1120,264,4221,1646,11,779,358,1541,956,617,21958,477,16024,1093,12966,656,13,4452,11,358,2846,31301,10489,323,5644,311,7945,499,449,904,4860,477,9256,499,1253,617,0,2650,649,358,1520,499,3432,30],\"total_duration\":2662482958,\"load_duration\":45539583,\"prompt_eval_count\":31,\"prompt_eval_duration\":641000000,\"eval_count\":47,\"eval_duration\":1971000000}\n",
            "\n",
            "Response 1: {'response': \"I 'm  just  a  language  model ,  so  I  don 't  have  emotions  or  feelings  like  humans  do .  However ,  I 'm  functioning  properly  and  ready  to  assist  you  with  any  questions  or  tasks  you  may  have !  How  can  I  help  you  today ?\"}\n",
            "\n",
            "üì° Raw API Response:\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.669191Z\",\"response\":\"Nice\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.710741Z\",\"response\":\" introduction\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.752897Z\",\"response\":\"!\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.794655Z\",\"response\":\" It\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.838665Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.883274Z\",\"response\":\" great\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.931527Z\",\"response\":\" that\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:56.979993Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.027028Z\",\"response\":\"'re\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.072295Z\",\"response\":\" acknowledging\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.114639Z\",\"response\":\" the\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.157375Z\",\"response\":\" limitations\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.203685Z\",\"response\":\" of\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.252385Z\",\"response\":\" being\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.296054Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.337704Z\",\"response\":\" machine\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.37984Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.422177Z\",\"response\":\" while\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.464405Z\",\"response\":\" also\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.50995Z\",\"response\":\" expressing\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.55731Z\",\"response\":\" enthusiasm\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.602412Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.647021Z\",\"response\":\" assist\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.694654Z\",\"response\":\".\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.738663Z\",\"response\":\"I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.781463Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.826768Z\",\"response\":\" an\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.873814Z\",\"response\":\" AI\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.916429Z\",\"response\":\" designed\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:57.961679Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.008044Z\",\"response\":\" simulate\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.051077Z\",\"response\":\" conversations\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.094776Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.13868Z\",\"response\":\" answer\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.18343Z\",\"response\":\" questions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.228954Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.273614Z\",\"response\":\" the\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.317116Z\",\"response\":\" best\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.360308Z\",\"response\":\" of\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.403437Z\",\"response\":\" my\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.448241Z\",\"response\":\" knowledge\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.491801Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.534661Z\",\"response\":\" You\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.583953Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.632317Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.686458Z\",\"response\":\" need\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.737735Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.783529Z\",\"response\":\" share\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.82788Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.872701Z\",\"response\":\" personal\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.919575Z\",\"response\":\" name\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:58.967606Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.013446Z\",\"response\":\" as\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.060209Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.107284Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.151815Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.194373Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.238391Z\",\"response\":\" any\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.28258Z\",\"response\":\" expectations\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.327364Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.372126Z\",\"response\":\" emotional\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.419493Z\",\"response\":\" connections\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.46219Z\",\"response\":\" with\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.503376Z\",\"response\":\" individuals\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.547986Z\",\"response\":\".\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.59605Z\",\"response\":\"Feel\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.641778Z\",\"response\":\" free\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.683852Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.726274Z\",\"response\":\" ask\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.769729Z\",\"response\":\" me\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.813247Z\",\"response\":\" anything\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.857839Z\",\"response\":\" that\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.899773Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.943417Z\",\"response\":\" on\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:46:59.988121Z\",\"response\":\" your\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.033696Z\",\"response\":\" mind\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.07775Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.123114Z\",\"response\":\" from\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.165429Z\",\"response\":\" general\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.208438Z\",\"response\":\" knowledge\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.255216Z\",\"response\":\" queries\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.296632Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.339188Z\",\"response\":\" more\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.381361Z\",\"response\":\" specific\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.424848Z\",\"response\":\" topics\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.486511Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.580365Z\",\"response\":\" language\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.637364Z\",\"response\":\" learning\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.682348Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.724545Z\",\"response\":\" writing\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.766244Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.807879Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.853502Z\",\"response\":\" even\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.899029Z\",\"response\":\" creative\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.943371Z\",\"response\":\" writing\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:00.985791Z\",\"response\":\" prompts\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.028143Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.070792Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.113973Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.15846Z\",\"response\":\" here\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.202755Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.245728Z\",\"response\":\" provide\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.28847Z\",\"response\":\" information\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.332952Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.379479Z\",\"response\":\" suggestions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.422019Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.463466Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.507612Z\",\"response\":\" assistance\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.551439Z\",\"response\":\" whenever\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.594658Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.63733Z\",\"response\":\" need\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.682377Z\",\"response\":\" it\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.72511Z\",\"response\":\"!\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.768898Z\",\"response\":\"What\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.823592Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.867127Z\",\"response\":\" the\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.910005Z\",\"response\":\" first\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.952066Z\",\"response\":\" topic\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:01.994392Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.039158Z\",\"response\":\" question\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.082409Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.125446Z\",\"response\":\"'d\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.168864Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.211706Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.257139Z\",\"response\":\" explore\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.300618Z\",\"response\":\"?\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:02.347569Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,40,364,76,1120,264,4221,1646,1174,779,358,1541,364,83,617,21958,477,16024,1093,12966,656,662,4452,1174,358,364,76,31301,10489,323,5644,311,7945,499,449,904,4860,477,9256,499,1253,617,758,2650,649,358,1520,499,3432,949,3639,374,701,836,30,128009,128006,78191,128007,271,46078,17219,0,1102,596,2294,430,499,2351,61708,279,9669,315,1694,264,5780,11,1418,1101,37810,36232,311,7945,382,40,2846,459,15592,6319,311,38553,21633,323,4320,4860,311,279,1888,315,856,6677,13,1472,1541,956,1205,311,4430,264,4443,836,11,439,358,1541,956,617,904,17078,477,14604,13537,449,7931,382,34027,1949,311,2610,757,4205,430,596,389,701,4059,11,505,4689,6677,20126,311,810,3230,13650,1093,4221,6975,11,4477,11,477,1524,11782,4477,52032,13,358,2846,1618,311,3493,2038,11,18726,11,323,13291,15716,499,1205,433,2268,3923,596,279,1176,8712,477,3488,499,4265,1093,311,13488,30],\"total_duration\":6005653875,\"load_duration\":40588167,\"prompt_eval_count\":79,\"prompt_eval_duration\":282000000,\"eval_count\":127,\"eval_duration\":5678000000}\n",
            "\n",
            "Response 2: {'response': \"Nice  introduction !  It 's  great  that  you 're  acknowledging  the  limitations  of  being  a  machine ,  while  also  expressing  enthusiasm  to  assist .\\n\\n I 'm  an  AI  designed  to  simulate  conversations  and  answer  questions  to  the  best  of  my  knowledge .  You  don 't  need  to  share  a  personal  name ,  as  I  don 't  have  any  expectations  or  emotional  connections  with  individuals .\\n\\n Feel  free  to  ask  me  anything  that 's  on  your  mind ,  from  general  knowledge  queries  to  more  specific  topics  like  language  learning ,  writing ,  or  even  creative  writing  prompts .  I 'm  here  to  provide  information ,  suggestions ,  and  assistance  whenever  you  need  it !\\n\\n What 's  the  first  topic  or  question  you 'd  like  to  explore ?\"}\n",
            "\n",
            "üì° Raw API Response:\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.49836Z\",\"response\":\"Here\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.548748Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.594818Z\",\"response\":\" one\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.63845Z\",\"response\":\":\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.683695Z\",\"response\":\" Why\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.730964Z\",\"response\":\" did\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.772942Z\",\"response\":\" the\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.816129Z\",\"response\":\" programmer\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.858542Z\",\"response\":\" quit\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.903512Z\",\"response\":\" his\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.949925Z\",\"response\":\" job\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:03.992908Z\",\"response\":\"?\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.037928Z\",\"response\":\" Because\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.080866Z\",\"response\":\" he\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.12441Z\",\"response\":\" didn\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.168992Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.212152Z\",\"response\":\" get\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.259507Z\",\"response\":\" arrays\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.305407Z\",\"response\":\"!\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.351365Z\",\"response\":\" (\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.399516Z\",\"response\":\"get\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.442388Z\",\"response\":\" it\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.486077Z\",\"response\":\"?)\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.530126Z\",\"response\":\" Seriously\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.577085Z\",\"response\":\" though\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.621848Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.666799Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.71021Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.752668Z\",\"response\":\" glad\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.801388Z\",\"response\":\" we\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.845352Z\",\"response\":\" could\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.900916Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.943542Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:04.985873Z\",\"response\":\" friendly\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.03176Z\",\"response\":\" conversation\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.078707Z\",\"response\":\" about\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.124057Z\",\"response\":\" my\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.168654Z\",\"response\":\" capabilities\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.216492Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.263678Z\",\"response\":\" limitations\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.307714Z\",\"response\":\" as\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.352224Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.395581Z\",\"response\":\" machine\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.442277Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.48778Z\",\"response\":\" Now\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.533369Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.578429Z\",\"response\":\" what\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.622229Z\",\"response\":\" would\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.667867Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.714006Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.758968Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.803506Z\",\"response\":\" talk\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.853069Z\",\"response\":\" about\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.907411Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:05.954745Z\",\"response\":\" ask\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.002546Z\",\"response\":\"?\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.046122Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.090772Z\",\"response\":\"'m\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.134271Z\",\"response\":\" all\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.182983Z\",\"response\":\" ears\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.227071Z\",\"response\":\" (\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.270479Z\",\"response\":\"or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.315262Z\",\"response\":\" rather\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.358716Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.403538Z\",\"response\":\" all\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.449605Z\",\"response\":\" text\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.494565Z\",\"response\":\").\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:06.542797Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,40,364,76,1120,264,4221,1646,1174,779,358,1541,364,83,617,21958,477,16024,1093,12966,656,662,4452,1174,358,364,76,31301,10489,323,5644,311,7945,499,449,904,4860,477,9256,499,1253,617,758,2650,649,358,1520,499,3432,949,29959,17219,758,1102,364,82,2294,430,499,364,265,61708,279,9669,315,1694,264,5780,1174,1418,1101,37810,36232,311,7945,662,358,364,76,459,15592,6319,311,38553,21633,323,4320,4860,311,279,1888,315,856,6677,662,1472,1541,364,83,1205,311,4430,264,4443,836,1174,439,358,1541,364,83,617,904,17078,477,14604,13537,449,7931,662,32833,1949,311,2610,757,4205,430,364,82,389,701,4059,1174,505,4689,6677,20126,311,810,3230,13650,1093,4221,6975,1174,4477,1174,477,1524,11782,4477,52032,662,358,364,76,1618,311,3493,2038,1174,18726,1174,323,13291,15716,499,1205,433,758,3639,364,82,279,1176,8712,477,3488,499,364,67,1093,311,13488,949,25672,757,264,22380,13,128009,128006,78191,128007,271,8586,596,832,25,8595,1550,279,48888,17257,813,2683,30,9393,568,3287,956,636,18893,0,320,456,433,10380,57606,3582,11,358,2846,16089,584,1436,617,264,11919,10652,922,856,17357,323,9669,439,264,5780,13,4800,11,1148,1053,499,1093,311,3137,922,477,2610,30,358,2846,682,25212,320,269,4856,11,682,1495,570],\"total_duration\":3770040541,\"load_duration\":49506916,\"prompt_eval_count\":214,\"prompt_eval_duration\":667000000,\"eval_count\":68,\"eval_duration\":3046000000}\n",
            "\n",
            "Response 3: {'response': \"Here 's  one :  Why  did  the  programmer  quit  his  job ?  Because  he  didn 't  get  arrays !  ( get  it ?)  Seriously  though ,  I 'm  glad  we  could  have  a  friendly  conversation  about  my  capabilities  and  limitations  as  a  machine .  Now ,  what  would  you  like  to  talk  about  or  ask ?  I 'm  all  ears  ( or  rather ,  all  text ).\"}\n",
            "üóëÔ∏è Token cache cleared.\n",
            "\n",
            "üì° Raw API Response:\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.15745Z\",\"response\":\"I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.20487Z\",\"response\":\" can\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.254828Z\",\"response\":\" provide\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.304452Z\",\"response\":\" information\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.354123Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.400486Z\",\"response\":\" entertainment\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.442997Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.486462Z\",\"response\":\" but\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.528615Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.573338Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.619655Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.665564Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.710717Z\",\"response\":\" personal\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.754571Z\",\"response\":\" memories\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.798126Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.841335Z\",\"response\":\" experiences\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.886562Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.933143Z\",\"response\":\" humans\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:07.976039Z\",\"response\":\" do\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.02157Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.064807Z\",\"response\":\" Each\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.109717Z\",\"response\":\" time\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.157201Z\",\"response\":\" you\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.207601Z\",\"response\":\" interact\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.255203Z\",\"response\":\" with\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.30392Z\",\"response\":\" me\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.350154Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.394667Z\",\"response\":\" it\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.440917Z\",\"response\":\"'s\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.486566Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.532518Z\",\"response\":\" new\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.574823Z\",\"response\":\" conversation\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.618939Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.661737Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.708502Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.752977Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.800354Z\",\"response\":\" retain\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.844515Z\",\"response\":\" any\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.889639Z\",\"response\":\" information\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.936854Z\",\"response\":\" from\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:08.981579Z\",\"response\":\" previous\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.024997Z\",\"response\":\" conversations\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.068931Z\",\"response\":\".\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.112928Z\",\"response\":\"However\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.158181Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.201091Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.243642Z\",\"response\":\" can\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.28727Z\",\"response\":\" use\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.329115Z\",\"response\":\" my\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.371974Z\",\"response\":\" training\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.416946Z\",\"response\":\" data\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.458842Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.501705Z\",\"response\":\" generate\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.543221Z\",\"response\":\" responses\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.588355Z\",\"response\":\" that\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.631234Z\",\"response\":\" are\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.672957Z\",\"response\":\" based\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.717348Z\",\"response\":\" on\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.759466Z\",\"response\":\" what\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.804014Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.845788Z\",\"response\":\"'ve\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.891813Z\",\"response\":\" been\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.936018Z\",\"response\":\" trained\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:09.978601Z\",\"response\":\" on\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.024588Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.066749Z\",\"response\":\" This\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.109103Z\",\"response\":\" means\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.151805Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.195185Z\",\"response\":\" can\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.241038Z\",\"response\":\":\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.2856Z\",\"response\":\"*\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.327984Z\",\"response\":\" Recall\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.372668Z\",\"response\":\" general\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.413697Z\",\"response\":\" knowledge\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.457476Z\",\"response\":\" on\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.500536Z\",\"response\":\" a\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.54363Z\",\"response\":\" wide\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.588331Z\",\"response\":\" range\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.63197Z\",\"response\":\" of\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.675582Z\",\"response\":\" topics\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.716639Z\",\"response\":\"\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.758598Z\",\"response\":\"*\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.803986Z\",\"response\":\" Understand\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.847953Z\",\"response\":\" natural\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.893786Z\",\"response\":\" language\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.938987Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:10.982943Z\",\"response\":\" respond\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.027343Z\",\"response\":\" accordingly\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.071315Z\",\"response\":\"\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.117446Z\",\"response\":\"*\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.160045Z\",\"response\":\" Generate\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.204173Z\",\"response\":\" text\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.246493Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.290766Z\",\"response\":\" answers\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.335462Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.377562Z\",\"response\":\" questions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.421011Z\",\"response\":\" based\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.464397Z\",\"response\":\" on\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.50746Z\",\"response\":\" my\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.552032Z\",\"response\":\" training\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.594663Z\",\"response\":\" data\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.640493Z\",\"response\":\"\\n\\n\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.682833Z\",\"response\":\"But\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.725088Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.769366Z\",\"response\":\" don\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.811089Z\",\"response\":\"'t\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.856655Z\",\"response\":\" have\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.89798Z\",\"response\":\" personal\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.939608Z\",\"response\":\" memories\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:11.98524Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.030843Z\",\"response\":\" emotions\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.07425Z\",\"response\":\",\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.118448Z\",\"response\":\" or\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.160728Z\",\"response\":\" experiences\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.208325Z\",\"response\":\" like\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.258284Z\",\"response\":\" humans\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.30478Z\",\"response\":\" do\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.346339Z\",\"response\":\".\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.389586Z\",\"response\":\" I\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.434844Z\",\"response\":\" exist\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.479866Z\",\"response\":\" solely\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.52446Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.569127Z\",\"response\":\" provide\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.615669Z\",\"response\":\" information\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.662032Z\",\"response\":\" and\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.705566Z\",\"response\":\" assist\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.751293Z\",\"response\":\" with\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.795259Z\",\"response\":\" tasks\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.839813Z\",\"response\":\" to\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.886857Z\",\"response\":\" the\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.935516Z\",\"response\":\" best\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:12.982046Z\",\"response\":\" of\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:13.025966Z\",\"response\":\" my\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:13.071195Z\",\"response\":\" abilities\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:13.119578Z\",\"response\":\"!\",\"done\":false}\n",
            "{\"model\":\"llama3.2\",\"created_at\":\"2025-03-02T23:47:13.165893Z\",\"response\":\"\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,9125,128007,271,38766,1303,33025,2696,25,6790,220,2366,18,271,128009,128006,882,128007,271,3923,656,499,6227,30,128009,128006,78191,128007,271,40,649,3493,2038,323,16924,11,719,358,1541,956,617,4443,19459,477,11704,1093,12966,656,13,9062,892,499,16681,449,757,11,433,596,264,502,10652,323,358,1541,956,14389,904,2038,505,3766,21633,382,11458,11,358,649,1005,856,4967,828,311,7068,14847,430,527,3196,389,1148,358,3077,1027,16572,389,13,1115,3445,358,649,1473,9,80640,4689,6677,389,264,7029,2134,315,13650,198,9,71994,5933,4221,323,6013,28178,198,9,20400,1495,323,11503,311,4860,3196,389,856,4967,828,271,4071,358,1541,956,617,4443,19459,11,21958,11,477,11704,1093,12966,656,13,358,3073,21742,311,3493,2038,323,7945,449,9256,311,279,1888,315,856,18000,0],\"total_duration\":6231972416,\"load_duration\":49034458,\"prompt_eval_count\":30,\"prompt_eval_duration\":163000000,\"eval_count\":136,\"eval_duration\":6012000000}\n",
            "\n",
            "Response 4: {'response': \"I  can  provide  information  and  entertainment ,  but  I  don 't  have  personal  memories  or  experiences  like  humans  do .  Each  time  you  interact  with  me ,  it 's  a  new  conversation  and  I  don 't  retain  any  information  from  previous  conversations .\\n\\n However ,  I  can  use  my  training  data  to  generate  responses  that  are  based  on  what  I 've  been  trained  on .  This  means  I  can :\\n\\n *  Recall  general  knowledge  on  a  wide  range  of  topics \\n *  Understand  natural  language  and  respond  accordingly \\n *  Generate  text  and  answers  to  questions  based  on  my  training  data \\n\\n But  I  don 't  have  personal  memories ,  emotions ,  or  experiences  like  humans  do .  I  exist  solely  to  provide  information  and  assist  with  tasks  to  the  best  of  my  abilities !\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimized KV Cache with deque for Efficient Context Management**\n",
        "\n",
        "This section introduces an improved approach to simulating Key-Value (KV) cache behavior for the Ollama API using collections.deque for efficient memory and token queue management.\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "- Store and manage a rolling buffer of past tokens to simulate conversational memory.\n",
        "\n",
        "- Uses collections.deque for efficient popping and appending, making it suitable for large-scale or real-time inference.\n",
        "\n",
        "**Key Methods**\n",
        "\n",
        "  - add_tokens(new_text)\n",
        "\n",
        "        Tokenizes input (split()).\n",
        "\n",
        "        Enqueues tokens.\n",
        "\n",
        "        Automatically discards oldest tokens if max_tokens is exceeded.\n",
        "\n",
        "  - get_cached_context()\n",
        "\n",
        "        Returns a string representation of the current token buffer.\n",
        "\n",
        "- **clear_cache()**\n",
        "\n",
        "        Empties the queue and prints a confirmation.\n",
        "\n",
        "**Why Use deque?**\n",
        "\n",
        "- deque provides O(1) complexity for appends and pops on both ends.\n",
        "\n",
        "- Ideal for simulating LRU (Least Recently Used) cache mechanisms."
      ],
      "metadata": {
        "id": "u520ZUhuJNeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from collections import deque\n",
        "\n",
        "class KVCache:\n",
        "    \"\"\"Optimized KV Cache for Ollama by storing and managing context efficiently.\"\"\"\n",
        "    def __init__(self, max_tokens=1024):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.token_queue = deque()\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Splits new text into tokens and manages the queue to maintain context length.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        # Truncate the queue if it exceeds the limit\n",
        "        while len(self.token_queue) > self.max_tokens:\n",
        "            self.token_queue.popleft()\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves stored tokens as a single string for inference.\"\"\"\n",
        "        return \" \".join(self.token_queue)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token queue.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "# Initialize KV Cache\n",
        "kv_cache = KVCache(max_tokens=1024)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"\n",
        "    Queries Ollama API with a KV cache mechanism.\n",
        "    \"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        # Join tokens into a full response\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "        kv_cache.add_tokens(final_response)  # Store the response in cache\n",
        "\n",
        "        return {\"response\": final_response}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Testing with KV Cache\n",
        "print(\"\\nüü¢ Running Query 1\")\n",
        "response1 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"Hello, how are you?\", kv_cache)\n",
        "print(\"\\nResponse 1:\", response1)\n",
        "\n",
        "print(\"\\nüü¢ Running Query 2\")\n",
        "response2 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"What is your name?\", kv_cache)\n",
        "print(\"\\nResponse 2:\", response2)\n",
        "\n",
        "print(\"\\nüü¢ Running Query 3\")\n",
        "response3 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"Tell me a joke.\", kv_cache)\n",
        "print(\"\\nResponse 3:\", response3)\n",
        "\n",
        "# Clearing Cache and Retesting\n",
        "kv_cache.clear_cache()\n",
        "print(\"\\nüü¢ Running Query 4 (After Cache Clear)\")\n",
        "response4 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"What do you remember?\", kv_cache)\n",
        "print(\"\\nResponse 4:\", response4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neLRv9YoloqZ",
        "outputId": "3b84779d-d779-4cee-cc22-88a6ccabde79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Running Query 1\n",
            "\n",
            "Response 1: {'response': \"I 'm  just  a  language  model ,  so  I  don 't  have  emotions  or  feelings  like  humans  do .  However ,  I 'm  functioning  properly  and  ready  to  assist  you  with  any  questions  or  tasks  you  may  have !  How  about  you ?  How 's  your  day  going  so  far ?\"}\n",
            "\n",
            "üü¢ Running Query 2\n",
            "\n",
            "Response 2: {'response': \"That 's  correct ,  I 'm  just  a  language  model ,  so  I  don 't  have  emotions  or  feelings  like  humans  do .  I 'm  designed  to  process  and  respond  to  text -based  inputs ,  but  I  don 't  have  subjective  experiences  or  personal  opinions .\\n\\n As  for  you ,  it  sounds  like  you 're  doing  great !  You 've  started  the  conversation  with  a  friendly  and  consider ate  tone ,  which  is  perfect  for  a  language  model  interaction .  However ,  I  don 't  have  a  name ,  as  I 'm  an  AI  designed  to  provide  information  and  assist  with  tasks .  I  exist  solely  to  help  users  like  you  with  your  queries .\\n\\n Let 's  get  started  on  whatever  task  or  question  you 'd  like  to  discuss !  What 's  on  your  mind ?\"}\n",
            "\n",
            "üü¢ Running Query 3\n",
            "\n",
            "Response 3: {'response': 'I  see  what \\'s  happening  here !  You \\'re  trying  to  engage  in  a  conversation ,  but  we  both  know  that  I \\'m  just  a  language  model ,  and  my  responses  are  generated  based  on  patterns  and  algorithms .\\n\\n Let \\'s  have  some  fun  with  this ,  though !  Here \\'s  a  joke :  Why  did  the  computer  go  to  the  doctor ?  Because  it  had  a  virus !  ( get  it ?)\\n\\n Now ,  I \\'ll  play  along .  If  I  were  to  respond  as  if  I  had  emotions  or  feelings ,  I  might  say  something  like ,  \" I \\'m  glad  we  can  have  a  fun  conversation  together !  It \\'s  not  often  that  I  get  to  engage  with  users  in  such  a  playful  way .\"\\n\\n But  let \\'s  be  real ,  I  don \\'t  truly  feel  anything .  I \\'m  just  here  to  provide  information  and  assist  with  tasks .  So ,  what \\'s  on  your  mind ?  Do  you  have  a  specific  question  or  topic  you \\'d  like  to  discuss ?'}\n",
            "üóëÔ∏è KV Cache Cleared.\n",
            "\n",
            "üü¢ Running Query 4 (After Cache Clear)\n",
            "\n",
            "Response 4: {'response': \"I  can  provide  information  and  entertainment ,  but  I  don 't  have  personal  memories  or  experiences  like  humans  do .  Each  time  you  interact  with  me ,  it 's  a  new  conversation  and  I  don 't  retain  any  information  from  previous  conversations .\\n\\n However ,  I  can  access  a  vast  amount  of  knowledge  and  information  that  has  been  added  to  my  training  data  since  my  last  update  in  December   202 3 .  This  means  I  can  provide  information  on  a  wide  range  of  topics ,  including  but  not  limited  to :\\n\\n *  General  knowledge  ( history ,  science ,  sports ,  music ,  etc .)\\n *  Language  translation \\n *  Writing  and  proof reading  assistance \\n *  Conversation  and  dialogue  generation \\n\\n If  you  have  any  specific  questions  or  topics  you 'd  like  to  discuss ,  feel  free  to  ask  me !\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Persistent KV Cache with File-Based Storage for LLM Context**\n",
        "\n",
        "This section builds on the earlier simulated KV cache by introducing file-based persistence, enabling conversational context to persist across sessions.\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "\n",
        "- Automatically save and load token data to/from a JSON file.\n",
        "\n",
        "**Key Additions**\n",
        "\n",
        "**Method\tand Purpose**\n",
        "\n",
        "- save_cache()\tSaves token queue to kv_cache.json\n",
        "\n",
        "- load_cache()\tLoads from file if present\n",
        "\n",
        "- clear_cache()\tClears queue and deletes cache file\n",
        "\n",
        "**Features**\n",
        "\n",
        "- Efficient memory using collections.deque.\n",
        "\n",
        "- File persistence via standard JSON file.\n",
        "\n",
        "- Truncates to max_tokens for safety and performance."
      ],
      "metadata": {
        "id": "RtGO8KvdHyy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save cache to file\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "\n",
        "class KVCache:\n",
        "    \"\"\"Optimized KV Cache with File Storage for Persistent Memory.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache.json\"\n",
        "\n",
        "    def __init__(self, max_tokens=1024):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.token_queue = deque()\n",
        "        self.load_cache()  # Load existing cache from file\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Splits new text into tokens, updates the queue, and saves to file.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        # Truncate the queue if it exceeds the limit\n",
        "        while len(self.token_queue) > self.max_tokens:\n",
        "            self.token_queue.popleft()\n",
        "\n",
        "        self.save_cache()  # Save updated cache to file\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves stored tokens as a single string for inference.\"\"\"\n",
        "        return \" \".join(self.token_queue)\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Saves the current cache to a JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(list(self.token_queue), file)\n",
        "            print(\"üíæ Cache saved to file.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads the cache from a JSON file if it exists.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    cached_tokens = json.load(file)\n",
        "                    self.token_queue = deque(cached_tokens[-self.max_tokens:])  # Load only up to max_tokens\n",
        "                print(\"üîÑ Cache loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading cache: {e}\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token queue and removes the cache file.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "# Initialize KV Cache with Persistent Storage\n",
        "kv_cache = KVCache(max_tokens=1024)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"\n",
        "    Queries Ollama API with a KV cache mechanism.\n",
        "    \"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        # Join tokens into a full response\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "        kv_cache.add_tokens(final_response)  # Store the response in cache\n",
        "\n",
        "        return {\"response\": final_response}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Testing with Persistent KV Cache\n",
        "print(\"\\nüü¢ Running Query 1\")\n",
        "response1 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"Hello, how are you?\", kv_cache)\n",
        "print(\"\\nResponse 1:\", response1)\n",
        "\n",
        "print(\"\\nüü¢ Running Query 2\")\n",
        "response2 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"What is your name?\", kv_cache)\n",
        "print(\"\\nResponse 2:\", response2)\n",
        "\n",
        "print(\"\\nüü¢ Running Query 3\")\n",
        "response3 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"Tell me a joke.\", kv_cache)\n",
        "print(\"\\nResponse 3:\", response3)\n",
        "\n",
        "# Clearing Cache and Retesting\n",
        "kv_cache.clear_cache()\n",
        "print(\"\\nüü¢ Running Query 4 (After Cache Clear)\")\n",
        "response4 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", \"What do you remember?\", kv_cache)\n",
        "print(\"\\nResponse 4:\", response4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xTDIVYHnJyl",
        "outputId": "24163308-fdf3-42cd-fe33-4badb9efc7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Cache loaded from file.\n",
            "\n",
            "üü¢ Running Query 1\n",
            "üíæ Cache saved to file.\n",
            "\n",
            "Response 1: {'response': \"I 'm  just  a  language  model ,  so  I  don 't  have  emotions  or  feelings  like  humans  do .  However ,  I 'm  functioning  properly  and  ready  to  assist  you  with  any  questions  or  tasks  you  may  have !  How  can  I  help  you  today ?\"}\n",
            "\n",
            "üü¢ Running Query 2\n",
            "üíæ Cache saved to file.\n",
            "\n",
            "Response 2: {'response': 'Thank  you  for  the  warm  introduction !  I \\'m  happy  to  chat  with  you ,  even  if  it \\'s  just  a  language  model  like  yourself .  I  don \\'t  have  a  personal  name ,  but  I \\'ll  refer  to  myself  as  \" Assistant \"  or  \" AI \"  from  now  on .\\n\\n I \\'m  here  to  help  answer  any  questions ,  provide  information ,  or  assist  with  tasks  you  may  have .  What \\'s  on  your  mind ?  Are  you  looking  for  help  with  something  specific ,  or  do  you  want  to  engage  in  a  fun  conversation ?'}\n",
            "\n",
            "üü¢ Running Query 3\n",
            "üíæ Cache saved to file.\n",
            "\n",
            "Response 3: {'response': \"It 's  great  to  meet  you  too !  I 'm  glad  to  be  of  assistance  and  happy  to  chat  with  you  about  anything  that  interests  you .\\n\\n Here 's  a  joke  to  get  us  started :\\n\\n Why  did  the  scare crow  win  an  award ?\\n\\n Because  he  was  outstanding  in  his  field !\\n\\n I  hope  that  made  you  smile !  Do  you  have  a  specific  topic  or  question  on  your  mind ,  or  would  you  like  me  to  suggest  some  conversation  starters ?\"}\n",
            "üóëÔ∏è KV Cache Cleared.\n",
            "\n",
            "üü¢ Running Query 4 (After Cache Clear)\n",
            "üíæ Cache saved to file.\n",
            "\n",
            "Response 4: {'response': \"I  can  provide  information  and  entertainment ,  but  I  don 't  have  personal  experiences  or  memories  like  humans  do .  Each  time  you  interact  with  me ,  it 's  a  new  conversation  and  I  don 't  retain  any  information  from  previous  conversations .\\n\\n However ,  I  can  continue  to  learn  and  improve  based  on  the  conversations  I  have  with  users  like  you .  This  means  that  if  you  ask  me  the  same  question  multiple  times ,  I 'll  do  my  best  to  provide  a  more  accurate  or  helpful  response .\\n\\n If  you 'd  like ,  we  can  start  fresh  and  explore  a  new  topic .  What 's  on  your  mind ?\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Measuring Cache File Growth with Fixed Prompt Size for 10 Inferences**\n",
        "\n",
        "**This section demonstrates how to:**\n",
        "\n",
        "- Simulate a KV cache with unlimited token storage.\n",
        "\n",
        "- Limit the number of tokens used for prompting to a fixed window (512 tokens).\n",
        "\n",
        "- Track the cache file size growth across multiple inferences.\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "- Maintain a growing memory of previous interactions.\n",
        "\n",
        "- Use only the latest prompt_size tokens (512) during inference.\n",
        "\n",
        "- Persist the entire token history to a JSON file.\n",
        "\n",
        "- Track and print cache file size after each inference.\n",
        "\n",
        "**Key Features**\n",
        "\n",
        "- add_tokens()\tAppends all new tokens to the queue and saves to disk.\n",
        "- get_cached_context()\tReturns only the latest N tokens (prompt_size).\n",
        "- save_cache()\tStores full cache and logs file size.\n",
        "- load_cache()\tLoads all past tokens (if any) from file.\n",
        "- clear_cache()\tDeletes both in-memory and file-based cache."
      ],
      "metadata": {
        "id": "Ra72sf1PK_Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#File size calculation with fixed prompt/token size for 10 inferences\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from collections import deque\n",
        "\n",
        "class KVCache:\n",
        "    \"\"\"KV Cache with Unlimited Storage & Fixed Prompt Size.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache.json\"\n",
        "\n",
        "    def __init__(self, prompt_size=512):\n",
        "        self.prompt_size = prompt_size  # Fixed prompt size\n",
        "        self.token_queue = deque()\n",
        "        self.load_cache()  # Load existing cache from file\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Adds new tokens to the cache with unlimited storage.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        self.save_cache()  # Save updated cache to file\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves only the latest 512 tokens for inference.\"\"\"\n",
        "        tokens = list(self.token_queue)[-self.prompt_size:]  # Always take the last 512 tokens\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Saves the current cache to a JSON file and logs file size.\"\"\"\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(list(self.token_queue), file)\n",
        "            file_size = os.path.getsize(self.CACHE_FILE)  # Get file size\n",
        "            print(f\"üíæ Cache saved. (Size: {file_size} bytes)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads the cache from a JSON file if it exists.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    cached_tokens = json.load(file)\n",
        "                    self.token_queue = deque(cached_tokens)  # Load full cache\n",
        "                print(\"üîÑ Cache loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading cache: {e}\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token queue and removes the cache file.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "# Initialize KV Cache with Fixed Prompt Size (512 Tokens)\n",
        "kv_cache = KVCache(prompt_size=512)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"\n",
        "    Queries Ollama API with a KV cache mechanism.\n",
        "    \"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        # Join tokens into a full response\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "        kv_cache.add_tokens(final_response)  # Store the response in cache\n",
        "\n",
        "        return {\"response\": final_response}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Run 10 Queries (No Cache Size Limit)\n",
        "for i in range(10):\n",
        "    print(f\"\\nüü¢ Running Query {i+1} (Fixed Prompt Size: 512 tokens, Unlimited Cache)\")\n",
        "    user_prompt = f\"This is test iteration {i+1}. Tell me something interesting.\"\n",
        "    response = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", user_prompt, kv_cache)\n",
        "    print(f\"\\nResponse {i+1}:\", response)\n",
        "\n",
        "    # Show file size after each iteration\n",
        "    if os.path.exists(\"kv_cache.json\"):\n",
        "        file_size = os.path.getsize(\"kv_cache.json\")\n",
        "        print(f\"üìÇ Cache File Size After Iteration {i+1}: {file_size} bytes\")\n",
        "\n",
        "# Clearing Cache at the End\n",
        "kv_cache.clear_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZsVSsFrU_bG",
        "outputId": "54350fe8-e5f6-404e-9547-03ccfbff8eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Running Query 1 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 970 bytes)\n",
            "\n",
            "Response 1: {'response': 'Iteration   1 ,  let \\'s  get  started .\\n\\n Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  Tur rit opsis  do hr n ii  theoretically  immortal .\\n\\n How \\'s  that  for  something  interesting ?'}\n",
            "üìÇ Cache File Size After Iteration 1: 970 bytes\n",
            "\n",
            "üü¢ Running Query 2 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 2711 bytes)\n",
            "\n",
            "Response 2: {'response': 'I \\'m  glad  you  found  the  first  tid bit  interesting !\\n\\n Here \\'s  something  new  and  fascinating :\\n\\n Did  you  know  about  the  \" M eth us elah  Tree \"?  It \\'s  a  ancient  br istle cone  pine  tree  ( Pin us  long a eva )  that  grows  in  the  White  Mountains  of  California ,  USA .  This  tree  is  estimated  to  be  around   4 , 855  years  old ,  making  it  one  of  the  oldest  living  organisms  on  Earth !\\n\\n To  put  its  age  into  perspective ,  when  this  tree  was  a  sap ling ,  the  ancient  Egyptian  py ramids  were  still  being  built .  The  tree  has  survived  countless  natural  disasters ,  including  floods ,  fires ,  and  even  earthquakes .\\n\\n What \\'s  even  more  remarkable  is  that  the  Meth us elah  Tree  is  still  growing ,  albeit  very  slowly .  Scientists  have  measured  its  growth  rate  to  be  around   1 - 2  inches  per  year ,  making  it  one  of  the  slow est -growing  trees  on  record .\\n\\n This  incredible  tree  has  been  dubbed  the  \" Queen  of  Trees \"  and  has  become  a  symbol  of  endurance  and  longevity .  Its  age  and  resilience  are  a  testament  to  the  incredible  power  and  adapt ability  of  life  on  Earth .\\n\\n How \\'s  that  for  something  interesting ?'}\n",
            "üìÇ Cache File Size After Iteration 2: 2711 bytes\n",
            "\n",
            "üü¢ Running Query 3 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 5502 bytes)\n",
            "\n",
            "Response 3: {'response': 'I \\'m  glad  you \\'re  excited  about  the  previous  tid bits !  For  this  new  iteration ,  I \\'d  like  to  share  something  interesting  with  you .\\n\\n Did  you  know  about  the  \" G low worm  C aves \"  in  Wait omo ,  New  Zealand ?  These  caves  are  home  to  a  unique  and  breathtaking  phenomenon  where  glow worm s  ( Ar ach noc ampa  lumin osa )  have  created  an  eth ereal  light  show .  The  glow worm s  emit  a  blue -green  lum ines cence  from  their  abdom ens ,  which  is  used  for  mating  and  defense .\\n\\n As  you  walk  through  the  caves ,  you \\'ll  see  a  canopy  of  twink ling  lights  above  you ,  creating  an  other world ly  atmosphere .  It \\'s  as  if  you \\'re  floating  through  a  magical  forest .  The  glow worm s  are  attracted  to  the  moisture  in  the  air  and  have  created  this  spectacular  display  over  millions  of  years .\\n\\n But  what \\'s  even  more  fascinating  is  that  the  Glow worm  C aves  were  once  a  major  highway  for  ancient  humans .  Archae ologists  have  discovered  evidence  of  human  hab itation  dating  back  around   1 , 000  years ,  with  some  areas  showing  signs  of  recent  human  activity .  This  has  led  to  speculation  about  whether  modern -day  explor ers  might  be  disturbing  the  habitat  of  these  glowing  creatures .\\n\\n Additionally ,  the  Glow worm  C aves  are  also  a  popular  destination  for  adventure  seekers  and  scientists  alike .  Researchers  have  been  studying  the  glow worm s  to  better  understand  their  biology  and  behavior ,  which  could  lead  to  breakthrough s  in  fields  like  medicine  and  conservation .\\n\\n The  Glow worm  C aves  are  an  awe -ins piring  natural  wonder  that  showcases  the  incredible  diversity  and  beauty  of  our  planet .  They \\'re  a  reminder  of  the  importance  of  preserving  our  natural  world  and  protecting  these  unique  ecosystems  for  future  generations  to  enjoy .\\n\\n What  do  you  think ?  Is  this  something  interesting  for  you ?'}\n",
            "üìÇ Cache File Size After Iteration 3: 5502 bytes\n",
            "\n",
            "üü¢ Running Query 4 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 8481 bytes)\n",
            "\n",
            "Response 4: {'response': 'That \\'s  quite  an  exciting  set  of  tid bits !  I \\'d  like  to  share  something  fascinating  with  you .\\n\\n Did  you  know  about  the  \" O jo  Cal iente  Minerals \"  in  New  Mexico ,  USA ?  This  natural  wonder  features  a  stunning  array  of  colorful  minerals  and  hot  springs  that  have  been  shaped  by  millions  of  years  of  geological  activity .  The  site  is  home  to  an  incredible  variety  of  minerals ,  including  quartz ,  am ethyst ,  and  ge odes ,  which  are  formed  when  hot  water  rich  in  minerals  flows  through  the  Earth \\'s  crust .\\n\\n The  most  remarkable  aspect  of  O jo  Cal iente  Minerals  is  the  \" Crystal  Cave ,\"  a  massive  underground  chamber  filled  with  towering  crystal  formations  that  have  been  growing  for  thousands  of  years .  The  crystals ,  some  of  which  reach  heights  of  over   100  feet ,  are  made  up  of  sodium  sulfate  and  other  minerals  that  have  been  deposited  over  time .\\n\\n What \\'s  even  more  fascinating  about  O jo  Cal iente  Minerals  is  its  connection  to  the  ancient  M eso american  civilizations .  Archae ologists  have  discovered  evidence  of  human  hab itation  dating  back  over   1 , 000  years ,  with  some  areas  showing  signs  of  recent  human  activity .  This  has  led  to  speculation  about  whether  modern -day  explor ers  might  be  disturbing  the  habitat  of  these  mineral  formations .\\n\\n In  addition  to  its  geological  and  historical  significance ,  O jo  Cal iente  Minerals  is  also  a  popular  destination  for  adventure  seekers  and  scientists  alike .  Researchers  have  been  studying  the  site  to  better  understand  the  formation  of  minerals  and  the  ge ology  of  the  region ,  which  could  lead  to  breakthrough s  in  fields  like  mining  and  conservation .\\n\\n The  natural  wonder  of  O jo  Cal iente  Minerals  serves  as  a  reminder  of  the  incredible  diversity  and  beauty  of  our  planet ,  and  its  connection  to  human  history  is  a  testament  to  the  importance  of  preserving  these  unique  ecosystems  for  future  generations  to  enjoy .\\n\\n How \\'s  that ?  Is  this  something  interesting  for  you ?'}\n",
            "üìÇ Cache File Size After Iteration 4: 8481 bytes\n",
            "\n",
            "üü¢ Running Query 5 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 9638 bytes)\n",
            "\n",
            "Response 5: {'response': \"I 'm  glad  you 're  excited  about  sharing  interesting  facts !  However ,  I  must  inform  you  that  the  text  you  provided  appears  to  be  a  copy -p asted  version  of  the  original  passage ,  with  some  modifications  and  additions .\\n\\n As  a  convers ational  AI ,  I  don 't  have  personal  preferences  or  interests  like  humans  do ,  but  I  can  certainly  help  facilitate  discussions  and  provide  information  on  various  topics .\\n\\n If  you 'd  like  to  share  something  interesting ,  I 'd  be  happy  to  hear  about  it !  Perhaps  you  could  tell  me  about  a  specific  topic ,  such  as  a  fascinating  scientific  discovery ,  an  intriguing  historical  event ,  or  an  awe -ins piring  natural  wonder ?\\n\\n Let 's  start  fresh  and  explore  new  topics  together .  What  would  you  like  to  talk  about ?\"}\n",
            "üìÇ Cache File Size After Iteration 5: 9638 bytes\n",
            "\n",
            "üü¢ Running Query 6 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 11271 bytes)\n",
            "\n",
            "Response 6: {'response': \"The  text  provided  appears  to  be  a  copy -p asted  version  of  the  original  passage ,  which  was  written  in  a  style  that  resembles  Wikipedia -style  articles  or  educational  content .  The  text  does  not  seem  to  have  any  personal  opinions  or  experiences  expressed  by  the  author .\\n\\n However ,  I 'm  happy  to  chat  with  you  about  something  interesting !  Since  you 're  enthusiastic  about  sharing  facts  and  information ,  I 'll  start  by  asking  you :  What  do  you  find  most  fascinating  about  the  natural  world ?  Are  there  any  particular  topics ,  such  as  astronomy ,  biology ,  or  ge ology ,  that  you  enjoy  learning  about ?\\n\\n If  not ,  we  could  explore  a  wide  range  of  interesting  topics  together .  Perhaps  we  could  discuss :\\n\\n 1 .  A  recent  scientific  breakthrough  or  discovery ?\\n 2 .  An  intriguing  historical  event  or  cultural  phenomenon ?\\n 3 .  An  awe -ins piring  natural  wonder  or  unique  ecosystem ?\\n 4 .  A  fascinating  technology  or  innovation ?\\n\\n Let  me  know  what  sparks  your  curiosity ,  and  I 'll  do  my  best  to  engage  in  a  conversation  that 's  both  informative  and  engaging !\"}\n",
            "üìÇ Cache File Size After Iteration 6: 11271 bytes\n",
            "\n",
            "üü¢ Running Query 7 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 12288 bytes)\n",
            "\n",
            "Response 7: {'response': 'One  interesting  fact  is  that  there  is  a  type  of  jelly fish  that  is  immortal .  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal .\\n\\n Would  you  like  to  hear  more  about  this  fascinating  creature  or  explore  other  interesting  topics ?'}\n",
            "üìÇ Cache File Size After Iteration 7: 12288 bytes\n",
            "\n",
            "üü¢ Running Query 8 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 13195 bytes)\n",
            "\n",
            "Response 8: {'response': 'I  see  what \\'s  happening  here !  It  seems  like  we \\'ve  got  a  playful  conversation  starter  that \\'s  been  copied  and  past ed ,  but  with  some  modifications .\\n\\n Let \\'s  start  fresh  and  talk  about  the  topic  at  hand :  \" jo  Cal  i ente  Minerals \"  seems  to  be  a  typo  or  an  incorrect  passage .  Can  you  tell  me  more  about  what  you \\'d  like  to  discuss  regarding  minerals ?  Are  there  any  specific  types  of  minerals ,  geological  formations ,  or  mining -related  topics  that  interest  you ?\\n\\n If  not ,  I \\'d  be  happy  to  suggest  some  interesting  mineral -related  topics  we  could  explore  together !'}\n",
            "üìÇ Cache File Size After Iteration 8: 13195 bytes\n",
            "\n",
            "üü¢ Running Query 9 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 14614 bytes)\n",
            "\n",
            "Response 9: {'response': 'I \\'m  glad  you \\'re  excited  to  start  a  new  conversation !  However ,  I  must  say  that  the  text  provided  seems  to  be  a  past ed  version  of  a  passage ,  and  it  appears  that  we \\'re  having  a  discussion  about  the  same  topic  multiple  times .\\n\\n Let \\'s  start  fresh !  You  mentioned  earlier  that  you \\'d  like  to  talk  about  something  interesting .  How  about  we  explore  the  topic  of  \" J ade \"?  Jade  is  a  type  of  metam orphic  rock  that  has  been  prized  for  its  vibrant  green  color  and  durability  for  thousands  of  years .  It \\'s  one  of  the  hardest  substances  known ,  with  a  Moh s  hardness  rating  of   6 . 5 - 7 . 0 .\\n\\n Would  you  like  to  learn  more  about  jade ,  its  properties ,  or  how  it \\'s  used  in  different  cultures ?  Or  perhaps  we  could  explore  other  interesting  topics  related  to  minerals ,  ge ology ,  or  natural  wonders ?\\n\\n I \\'m  happy  to  engage  in  a  conversation  that \\'s  both  informative  and  engaging !'}\n",
            "üìÇ Cache File Size After Iteration 9: 14614 bytes\n",
            "\n",
            "üü¢ Running Query 10 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 16984 bytes)\n",
            "\n",
            "Response 10: {'response': 'I \\'m  glad  you \\'re  excited  to  start  a  new  conversation !\\n\\n Let \\'s  talk  about  \" J ade \".  Jade  is  indeed  a  fascinating  mineral ,  and  its  cultural  significance  spans  thousands  of  years .\\n\\n Did  you  know  that  the  name  \" J ade \"  comes  from  the  Greek  word  \" i ath ys \",  which  means  \" he aling  stone \"?  In  ancient  China ,  jade  was  considered  a  symbol  of  good  fortune ,  prosperity ,  and  longevity .  The  Chinese  believed  that  jade  had  healing  properties  and  could  ward  off  evil  spirits .\\n\\n J ade  is  also  highly  prized  for  its  vibrant  green  color ,  which  can  range  from  pale  to  deep  em erald  tones .  It \\'s  often  used  in  high -end  jewelry  and  decorative  objects  because  of  its  durability  and  resistance  to  scratches .\\n\\n Interestingly ,  there  are  several  types  of  jade ,  including :\\n\\n 1 .  N eph rite  jade :  This  is  the  most  common  type  of  jade  and  is  found  in  China ,  Russia ,  and  the  United  States .\\n 2 .  Jade ite  jade :  This  type  of  jade  is  r arer  and  more  valuable  than  neph rite  jade ,  with  a  more  vibrant  green  color .\\n 3 .  Imperial  jade :  This  type  of  jade  is  highly  prized  for  its  intense  blue -green  color  and  is  often  used  in  high -end  jewelry .\\n\\n J ade  has  also  played  an  important  role  in  many  cultures  throughout  history .  In  ancient  Egypt ,  it  was  associated  with  the  goddess  Isis ,  while  in  Mes opot am ia ,  it  was  believed  to  have  magical  powers .\\n\\n Would  you  like  to  learn  more  about  jade \\'s  cultural  significance  or  explore  other  interesting  topics  related  to  minerals  and  ge ology ?'}\n",
            "üìÇ Cache File Size After Iteration 10: 16984 bytes\n",
            "üóëÔ∏è KV Cache Cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Caching with Inference Time Tracking ‚Äì Fixed Prompt, Unlimited History**\n",
        "This block enhances the persistent KV cache system by introducing inference time measurement for each API call, while still using a fixed prompt window and an unlimited cache history saved to file.\n",
        "\n",
        "**Key Enhancements**\n",
        "\n",
        "- Stores all past tokens (no pruning).\n",
        "\n",
        "- Always returns only the last prompt_size tokens (e.g., 512) for prompting.\n",
        "\n",
        "- Measures and prints the size of the cache file after each update.\n",
        "\n",
        "**Prompt Construction**\n",
        "\n",
        "Concatenates the last 512 tokens with the user input.\n",
        "\n",
        "**Timing Start**\n",
        "\n",
        "Records current time with start_time = time.time().\n",
        "\n",
        "**Streaming API Call**\n",
        "\n",
        "Parses streamed response as usual.\n",
        "\n",
        "**Timing End & Delta**\n",
        "\n",
        "Captures elapsed time as inference_time.\n",
        "\n",
        "**Cache Update**\n",
        "\n",
        "Response is added to the unlimited cache and saved to file.\n"
      ],
      "metadata": {
        "id": "YxsxiTlTL9Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "class KVCache:\n",
        "    \"\"\"KV Cache with Unlimited Storage & Fixed Prompt Size.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache.json\"\n",
        "\n",
        "    def __init__(self, prompt_size=512):\n",
        "        self.prompt_size = prompt_size  # Fixed prompt size\n",
        "        self.token_queue = deque()\n",
        "        self.load_cache()  # Load existing cache from file\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Adds new tokens to the cache with unlimited storage.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        self.save_cache()  # Save updated cache to file\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves only the latest 512 tokens for inference.\"\"\"\n",
        "        tokens = list(self.token_queue)[-self.prompt_size:]  # Always take the last 512 tokens\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Saves the current cache to a JSON file and logs file size.\"\"\"\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(list(self.token_queue), file)\n",
        "            file_size = os.path.getsize(self.CACHE_FILE)  # Get file size\n",
        "            print(f\"üíæ Cache saved. (Size: {file_size} bytes)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads the cache from a JSON file if it exists.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    cached_tokens = json.load(file)\n",
        "                    self.token_queue = deque(cached_tokens)  # Load full cache\n",
        "                print(\"üîÑ Cache loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading cache: {e}\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token queue and removes the cache file.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "# Initialize KV Cache with Fixed Prompt Size (512 Tokens)\n",
        "kv_cache = KVCache(prompt_size=512)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"\n",
        "    Queries Ollama API with a KV cache mechanism and measures inference time.\n",
        "    \"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    start_time = time.time()  # Start measuring inference time\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        end_time = time.time()  # End measuring inference time\n",
        "        inference_time = end_time - start_time  # Compute elapsed time\n",
        "\n",
        "        # Join tokens into a full response\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "        kv_cache.add_tokens(final_response)  # Store the response in cache\n",
        "\n",
        "        return {\"response\": final_response, \"inference_time\": inference_time}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Run 10 Queries (No Cache Size Limit)\n",
        "for i in range(10):\n",
        "    print(f\"\\nüü¢ Running Query {i+1} (Fixed Prompt Size: 512 tokens, Unlimited Cache)\")\n",
        "\n",
        "    user_prompt = f\"This is test iteration {i+1}. Tell me something interesting.\"\n",
        "\n",
        "    response = query_ollama_with_cache(OLLAMA_API_URL, \"llama3.2\", user_prompt, kv_cache)\n",
        "\n",
        "    print(f\"\\nResponse {i+1}: {response['response']}\")\n",
        "    print(f\"‚è± Inference Time for Query {i+1}: {response['inference_time']:.4f} seconds\")\n",
        "\n",
        "    # Show file size after each iteration\n",
        "    if os.path.exists(\"kv_cache.json\"):\n",
        "        file_size = os.path.getsize(\"kv_cache.json\")\n",
        "        print(f\"üìÇ Cache File Size After Iteration {i+1}: {file_size} bytes\")\n",
        "\n",
        "# Clearing Cache at the End\n",
        "kv_cache.clear_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MdpMrDMYOFL",
        "outputId": "2632aa2b-149f-4b4f-f77f-41be4cfea22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Running Query 1 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 970 bytes)\n",
            "\n",
            "Response 1: Iteration   1 ,  let 's  get  started !\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal !\n",
            "\n",
            " How 's  that  for  an  interesting  fact ?\n",
            "‚è± Inference Time for Query 1: 9.9436 seconds\n",
            "üìÇ Cache File Size After Iteration 1: 970 bytes\n",
            "\n",
            "üü¢ Running Query 2 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 2740 bytes)\n",
            "\n",
            "Response 2: I 'm  excited  to  learn  about  the  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish .\"  This  species  has  gained  significant  attention  in  recent  years  due  to  its  unique  ability  to  cheat  death .\n",
            "\n",
            " Here 's  an  interesting  fact :  Did  you  know  that  there  is  a  type  of  fungus  that  can  control  the  minds  of  ants ?  O phi oc ord y ce ps  unilateral is ,  a  fungus  that  grows  in  the  rain fore sts  of  South  America ,  infect s  ants  and  takes  control  of  their  bodies .  The  fungus  eventually  kills  the  ant ,  but  not  before  causing  it  to  climb  to  a  high  location  and  bite  onto  a  leaf ,  creating  a  perfect  environment  for  the  fungus  to  produce  fruit ing  bodies  and  dis perse  its  sp ores .\n",
            "\n",
            " This  phenomenon  is  known  as  \" z ombie  ants ,\"  and  it 's  a  fascinating  example  of  how  one  organism  can  manipulate  another  to  further  its  own  survival .  The  fungus  essentially  turns  the  ant  into  a  \" mind less  drone \"  that  helps  spread  the  fungus 's  sp ores ,  ensuring  the  fungus 's  propagation  and  survival .\n",
            "\n",
            " What  do  you  think ?  Would  you  like  to  learn  more  about  this  phenomenon  or  explore  other  interesting  facts ?\n",
            "‚è± Inference Time for Query 2: 11.2485 seconds\n",
            "üìÇ Cache File Size After Iteration 2: 2740 bytes\n",
            "\n",
            "üü¢ Running Query 3 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 5019 bytes)\n",
            "\n",
            "Response 3: I  see  we 're  in  for  a  treat !  Let 's  dive  into  some  fascinating  facts .\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  sea  cucumber  called  the  \" sea  cucumber \"  ( Hol oth uria  pol yp idea )  that  can  exp el  its  internal  organs  as  a  defense  mechanism  and  then  reg row  them ?  This  process  is  called  \" aut otomy ,\"  and  it  allows  the  sea  cucumber  to  distract  predators  and  escape .  What 's  even  more  remarkable  is  that  it  can  also  regenerate  its  entire  body  from  a  small  piece  of  tissue .\n",
            "\n",
            " Another  interesting  fact  is  about  the  \" D uck -B illed  Plat yp us .\"  This  unique  creature  has  a  venom ous  spur  on  its  hind  leg ,  which  contains  a  powerful  neuro to xin  called  def ens in -like  peptide  ( D LP ).  The  plat yp us  uses  this  venom  to  defend  itself  from  predators  and  even  other  male  plat yp uses  during  mating  season .  The  venom  is  so  potent  that  it  can  kill  a  human  being  if  not  treated  promptly .\n",
            "\n",
            " Lastly ,  did  you  know  about  the  \" M im ic  Oct opus \"?  This  incredible  cep hal op od  has  specialized  cells  called  chromat oph ores  in  its  skin  that  allow  it  to  change  color  and  texture  to  mimic  other  sea  creatures ,  such  as  sea  snakes  or  lion fish .  The  oct opus  can  even  make  sounds  like  a  his sing  snake  to  confuse  predators !  It 's  an  amazing  example  of  adapt ability  and  camouflage .\n",
            "\n",
            " How 's  your  curiosity  level  now ?  Would  you  like  to  explore  more  fascinating  facts  like  these  or  perhaps  learn  about  something  specific ?\n",
            "‚è± Inference Time for Query 3: 15.4956 seconds\n",
            "üìÇ Cache File Size After Iteration 3: 5019 bytes\n",
            "\n",
            "üü¢ Running Query 4 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 7299 bytes)\n",
            "\n",
            "Response 4: It  looks  like  we 're  in  for  a  treat  indeed !\n",
            "\n",
            " I 'm  excited  to  share  some  fascinating  facts  with  you ,  and  I 'll  try  to  keep  the  interesting ness  level  up .\n",
            "\n",
            " Here 's  an  interesting  fact :  Did  you  know  that  there  is  a  type  of  shark  called  the  \"  Goblin  Shark \"  ( M its uk ur ina  ow ston i )?  It  has  a  unique  appearance ,  with  a  fl abby  body ,  small  teeth ,  and  a  distinctive  pink ish -b rown  color .  But  what 's  even  more  fascinating  about  this  shark  is  its  ability  to  detect  electricity  in  the  water !  The  g oblin  shark  has  specialized  electro re ceptors  called  amp ull ae  of  Loren z ini ,  which  allow  it  to  navigate  through  the  ocean  and  find  prey .\n",
            "\n",
            " I 'll  keep  going  with  some  more  interesting  facts !\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  lake  in  Africa  called  Lake  Nat ron ?  It 's  a  salt water  lake  that 's  so  alkal ine  that  it  can  turn  animals  into  stone !  The  high  pH  level  of  the  lake  is  due  to  the  presence  of  sodium  carbonate ,  which  creates  an  environment  that 's  hostile  to  most  living  organisms .  However ,  some  unique  micro organisms  have  adapted  to  this  extreme  environment ,  making  Lake  Nat ron  one  of  the  most  fascinating  and  inh osp itable  places  on  Earth .\n",
            "\n",
            " I 'll  stop  here  for  now ,  but  I  have  plenty  more  interesting  facts  up  my  sleeve !\n",
            "\n",
            " Would  you  like  to  explore  more  fascinating  facts ,  learn  about  a  specific  topic ,  or  perhaps  play  a  game ?  Let  me  know ,  and  we  can  have  some  fun !\n",
            "‚è± Inference Time for Query 4: 16.9624 seconds\n",
            "üìÇ Cache File Size After Iteration 4: 7299 bytes\n",
            "\n",
            "üü¢ Running Query 5 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 9287 bytes)\n",
            "\n",
            "Response 5: I 'm  happy  to  help  with  your  request ,  but  I  need  to  clarify  that  the  text  you  provided  seems  to  be  a  collection  of  interesting  facts  about  various  creatures  and  natural  phenomena ,  rather  than  a  single  prompt  or  question .  However ,  I  can  certainly  engage  in  a  conversation  about  these  fascinating  topics !\n",
            "\n",
            " If  you 'd  like  to  explore  more  interesting  facts ,  I  can  suggest  some  categories  or  topics .  For  example ,  we  could  talk  about :\n",
            "\n",
            " 1 .  Unique  adaptations  in  animals ,  such  as  the  sea  cucumber 's  reg enerative  abilities  or  the  plat yp us 's  venom ous  spur .\n",
            " 2 .  M ysterious  and  hidden  places  on  Earth ,  like  Lake  Nat ron 's  extreme  environment .\n",
            " 3 .  Incredible  creatures  with  special  abilities ,  such  as  the  mimic  oct opus 's  camouflage  skills  or  the  g oblin  shark 's  electro re ception .\n",
            "\n",
            " Or  we  could  play  a  game  related  to  these  topics ,  such  as :\n",
            "\n",
            " 1 .  \" Guess  the  creature \":  I 'll  describe  an  animal ,  and  you  try  to  guess  what  it  is .\n",
            " 2 .  \" Trivia  challenge \":  I 'll  ask  you  questions  about  the  creatures  or  natural  phenomena  mentioned  in  the  text .\n",
            " 3 .  \" Word  association \":  I 'll  give  you  a  word  related  to  one  of  the  topics ,  and  you  respond  with  a  word  that 's  associated  with  it .\n",
            "\n",
            " Which  option  sounds  interesting  to  you ?\n",
            "‚è± Inference Time for Query 5: 16.7903 seconds\n",
            "üìÇ Cache File Size After Iteration 5: 9287 bytes\n",
            "\n",
            "üü¢ Running Query 6 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 10922 bytes)\n",
            "\n",
            "Response 6: I 'd  love  to  explore  more  fascinating  facts  with  you !  Here 's  an  interesting  fact  to  get  us  started :  Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  into  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " Which  option  sounds  interesting  to  you ?  Would  you  like  to :\n",
            "\n",
            " 1 .  Explore  more  fascinating  facts  about  unique  adaptations  in  animals ?\n",
            " 2 .  Dive  into  mysterious  and  hidden  places  on  Earth ?\n",
            " 3 .  Discover  incredible  creatures  with  special  abilities ?\n",
            " 4 .  Play  one  of  the  games  I  mentioned ,  such  as  \" Guess  the  creature \"  or  \" Trivia  challenge \"?\n",
            "\n",
            " Let  me  know ,  and  we  can  have  some  fun  exploring  these  fascinating  topics  together !\n",
            "‚è± Inference Time for Query 6: 13.4859 seconds\n",
            "üìÇ Cache File Size After Iteration 6: 10922 bytes\n",
            "\n",
            "üü¢ Running Query 7 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 11927 bytes)\n",
            "\n",
            "Response 7: I  see  that  you 're  a  treasure  tro ve  of  fascinating  facts !  I 'd  love  to  explore  more  of  them  with  you .\n",
            "\n",
            " Here 's  a  new ,  unrelated  fact :  Did  you  know  that  there 's  a  type  of  fungus  called  Arm ill aria  ost oy ae ,  also  known  as  the  \" h oney  mushroom \"?  It 's  a  massive  fungal  colony  that  covers  over   9 . 6  square  kilometers  ( 3 . 7  sq  mi )  in  Oregon ,  USA !  What  makes  it  even  more  fascinating  is  that  it 's  estimated  to  be  around   2 , 400  years  old ,  making  it  one  of  the  oldest  living  organisms  on  Earth .\n",
            "\n",
            " Now ,  I 'd  love  to  ask :  Would  you  like  to  play  a  game  or  explore  a  specific  topic ?\n",
            "‚è± Inference Time for Query 7: 9.7192 seconds\n",
            "üìÇ Cache File Size After Iteration 7: 11927 bytes\n",
            "\n",
            "üü¢ Running Query 8 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 14435 bytes)\n",
            "\n",
            "Response 8: I 'm  excited  to  engage  with  you  and  share  some  fascinating  facts  about  unique  adaptations  in  animals ,  mysterious  and  hidden  places  on  Earth ,  incredible  creatures  with  special  abilities ,  and  more .\n",
            "\n",
            " Let 's  start  by  exploring  the  \" imm ortal  jelly fish \"  fact  you  shared !  The  Tur rit opsis  do hr n ii  is  indeed  a  remarkable  creature  that  can  transform  its  body  into  a  younger  state  through  trans different iation .  This  process  allows  it  to  bypass  the  normal  process  of  aging  and  death ,  making  it  theoretically  immortal .\n",
            "\n",
            " Now ,  let 's  get  back  to  our  conversation  options :\n",
            "\n",
            " 1 .  Unique  adaptations  in  animals :  I 'm  intrigued  by  the  sea  cucumber 's  reg enerative  abilities  and  the  plat yp us 's  venom ous  spur .\n",
            " 2 .  M ysterious  and  hidden  places  on  Earth :  Lake  Nat ron 's  extreme  environment  is  fascinating ,  with  its  high  pH  levels  and  unique  ecosystems .\n",
            " 3 .  Incredible  creatures  with  special  abilities :  The  mimic  oct opus 's  camouflage  skills  and  the  g oblin  shark 's  electro re ception  are  both  impressive .\n",
            "\n",
            " Which  of  these  topics  would  you  like  to  explore  first ?\n",
            "\n",
            " Or ,  if  you 'd  prefer ,  we  can  play  a  game !  Here  are  our  options :\n",
            "\n",
            " 1 .  \" Guess  the  creature \":  I 'll  describe  an  animal ,  and  you  try  to  guess  what  it  is .\n",
            " 2 .  \" Trivia  challenge \":  I 'll  ask  you  questions  about  unique  adaptations  in  animals ,  mysterious  places  on  Earth ,  or  incredible  creatures  with  special  abilities .\n",
            " 3 .  \" Word  association \":  I 'll  give  you  a  word  related  to  one  of  these  topics ,  and  you  respond  with  a  word  that 's  associated  with  it .\n",
            "\n",
            " Let  me  know  which  option  you  prefer !\n",
            "‚è± Inference Time for Query 8: 19.0703 seconds\n",
            "üìÇ Cache File Size After Iteration 8: 14435 bytes\n",
            "\n",
            "üü¢ Running Query 9 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 15714 bytes)\n",
            "\n",
            "Response 9: I 'm  excited  to  explore  the  fascinating  world  of  unique  adaptations  in  animals ,  mysterious  and  hidden  places  on  Earth ,  incredible  creatures  with  special  abilities ,  and  more !\n",
            "\n",
            " Let 's  dive  into  a  game  that  combines  all  these  topics !  How  about  we  play  \" Guess  the  creature \"?  I 'll  describe  an  animal ,  and  you  try  to  guess  what  it  is .  Then ,  if  you 're  ready ,  we  can  move  on  to  one  of  the  other  options :  Tr ivia  challenge  or  Word  association .\n",
            "\n",
            " Here 's  your  first  description :\n",
            "\n",
            " This  animal  has  a  unique  ability  to  turn  its  body  into  a  \" blo ody \"  pulp  to  confuse  predators  and  distract  them  while  it  escapes .  It  also  has  a  symb iotic  relationship  with  certain  bacteria  that  aid  in  its  digestion  process .  What  is  this  incredible  creature ?\n",
            "\n",
            " Take  a  guess ,  and  we 'll  see  how  close  you  get !\n",
            "‚è± Inference Time for Query 9: 11.0572 seconds\n",
            "üìÇ Cache File Size After Iteration 9: 15714 bytes\n",
            "\n",
            "üü¢ Running Query 10 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 16737 bytes)\n",
            "\n",
            "Response 10: I 'd  love  to  play  \" Guess  the  creature \"  with  you !  Based  on  your  description ,  I 'm  going  to  take  a  guess  that  the  animal  you 're  thinking  of  is  the  Ax ol ot l  ( Amb yst oma  mex ican um ).  Ax ol ot ls  are  known  for  their  ability  to  reg row  their  limbs  and  parts  of  their  brain ,  and  they  can  also  release  a  cloud  of  blood -like  fluid  from  their  bodies  to  confuse  predators .  They  also  have  a  unique  digestive  system  that  involves  symb iotic  bacteria .  Am  I  correct ?\n",
            "\n",
            " Now ,  if  you 're  ready ,  we  can  move  on  to  one  of  the  other  options :  Tr ivia  challenge  or  Word  association .  Let  me  know  which  one  you 'd  prefer !\n",
            "‚è± Inference Time for Query 10: 9.8360 seconds\n",
            "üìÇ Cache File Size After Iteration 10: 16737 bytes\n",
            "üóëÔ∏è KV Cache Cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference Time Comparison: With vs. Without KV Cache**\n",
        "This experiment benchmarks and compares the inference latency of a language model when using a simulated KV cache versus not using it. It also tracks how the persistent cache grows in file size.\n",
        "\n",
        "**Key Features**\n",
        "\n",
        "- Unlimited token storage via deque\n",
        "\n",
        "- Fixed prompt window of latest 512 tokens\n",
        "\n",
        "- Automatic saving/loading to/from kv_cache.json\n",
        "\n",
        "- File size monitoring after each inference\n",
        "\n",
        "- Manual cache reset with .clear_cache()\n",
        "\n",
        "**Inference Time Tracking**\n",
        "\n",
        "- Uses time.time() to calculate latency.\n",
        "\n",
        "- Returns both the model response and the time elapsed\n",
        "\n",
        "Clears memory and deletes kv_cache.json.\n",
        "\n",
        "Ensures reproducibility for future experiments."
      ],
      "metadata": {
        "id": "ZqRNynXrZnW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "class KVCache:\n",
        "    \"\"\"KV Cache with Unlimited Storage & Fixed Prompt Size.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache.json\"\n",
        "\n",
        "    def __init__(self, prompt_size=512):\n",
        "        self.prompt_size = prompt_size  # Fixed prompt size\n",
        "        self.token_queue = deque()\n",
        "        self.load_cache()  # Load existing cache from file\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Adds new tokens to the cache with unlimited storage.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "        self.save_cache()  # Save updated cache to file\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves only the latest 512 tokens for inference.\"\"\"\n",
        "        tokens = list(self.token_queue)[-self.prompt_size:]  # Always take the last 512 tokens\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Saves the current cache to a JSON file and logs file size.\"\"\"\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(list(self.token_queue), file)\n",
        "            file_size = os.path.getsize(self.CACHE_FILE)  # Get file size\n",
        "            print(f\"üíæ Cache saved. (Size: {file_size} bytes)\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads the cache from a JSON file if it exists.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    cached_tokens = json.load(file)\n",
        "                    self.token_queue = deque(cached_tokens)  # Load full cache\n",
        "                print(\"üîÑ Cache loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading cache: {e}\")\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears the token queue and removes the cache file.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "# Initialize KV Cache with Fixed Prompt Size (512 Tokens)\n",
        "kv_cache = KVCache(prompt_size=512)\n",
        "\n",
        "def query_ollama(api_url, model_name, user_input, use_cache=True):\n",
        "    \"\"\"\n",
        "    Queries Ollama API with or without KV caching and measures inference time.\n",
        "    \"\"\"\n",
        "    if use_cache:\n",
        "        cached_context = kv_cache.get_cached_context()\n",
        "        full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "    else:\n",
        "        full_prompt = user_input  # No cache is used\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    start_time = time.time()  # Start measuring inference time\n",
        "\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        end_time = time.time()  # End measuring inference time\n",
        "        inference_time = end_time - start_time  # Compute elapsed time\n",
        "\n",
        "        # Join tokens into a full response\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "\n",
        "        if use_cache:\n",
        "            kv_cache.add_tokens(final_response)  # Store the response in cache\n",
        "\n",
        "        return {\"response\": final_response, \"inference_time\": inference_time}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://d3ab-84-224-189-77.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "# Run 10 Queries (With & Without Cache)\n",
        "for i in range(10):\n",
        "    print(f\"\\nüü¢ Running Query {i+1} (Fixed Prompt Size: 512 tokens, Unlimited Cache)\")\n",
        "\n",
        "    user_prompt = f\"This is test iteration {i+1}. Tell me something interesting.\"\n",
        "\n",
        "    # With KV Caching\n",
        "    response_with_cache = query_ollama(OLLAMA_API_URL, \"llama3.2\", user_prompt, use_cache=True)\n",
        "    print(f\"\\n‚úÖ With KV Cache - Response {i+1}: {response_with_cache['response']}\")\n",
        "    print(f\"‚è± Inference Time with Cache: {response_with_cache['inference_time']:.4f} seconds\")\n",
        "\n",
        "    # Without KV Caching\n",
        "    response_without_cache = query_ollama(OLLAMA_API_URL, \"llama3.2\", user_prompt, use_cache=False)\n",
        "    print(f\"\\n‚ùå Without KV Cache - Response {i+1}: {response_without_cache['response']}\")\n",
        "    print(f\"‚è± Inference Time without Cache: {response_without_cache['inference_time']:.4f} seconds\")\n",
        "\n",
        "    # Show file size after each iteration\n",
        "    if os.path.exists(\"kv_cache.json\"):\n",
        "        file_size = os.path.getsize(\"kv_cache.json\")\n",
        "        print(f\"üìÇ Cache File Size After Iteration {i+1}: {file_size} bytes\")\n",
        "\n",
        "# Clearing Cache at the End\n",
        "kv_cache.clear_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ3wUTfweBjS",
        "outputId": "21d532ad-833a-46bf-aa88-3f87db37d3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¢ Running Query 1 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 763 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 1: Iteration   1  complete !\n",
            "\n",
            " Here 's  something  interesting :\n",
            "\n",
            " Did  you  know  that  there  is  a  species  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again ,  making  it  theoretically  immortal .\n",
            "\n",
            " Would  you  like  me  to  share  more  interesting  facts ?\n",
            "‚è± Inference Time with Cache: 8.3453 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 1: Iteration   1 ,  a  new  beginning !\n",
            "\n",
            " Here 's  something  interesting :\n",
            "\n",
            " Did  you  know  that  there  is  a  species  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  type  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again ,  making  it  theoretically  immortal .\n",
            "\n",
            " Isn 't  that  mind -b low ing ?\n",
            "‚è± Inference Time without Cache: 5.1396 seconds\n",
            "üìÇ Cache File Size After Iteration 1: 763 bytes\n",
            "\n",
            "üü¢ Running Query 2 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 2077 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 2: That 's  fascinating  about  the  Tur rit opsis  do hr n ii !  It 's  indeed  a  remarkable  species .\n",
            "\n",
            " For  Iter ation   2 ,  I 'd  love  to  share  some  interesting  facts  with  you .  Did  you  know  that  there  is  a  type  of  fungus  called  O phi oc ord y ce ps  unilateral is ,  also  known  as  the  \" z ombie  fungus \"?  This  fungus  infect s  ants  and  takes  control  of  their  bodies ,  causing  them  to  climb  to  high  locations  and  bite  onto  a  leaf  before  producing  sp ores  that  can  spread  the  fungus .\n",
            "\n",
            " What 's  even  more  remarkable  about  this  fungus  is  that  it  can  manipulate  the  ant 's  behavior  to  ensure  its  own  survival .  The  fungus  essentially  turns  the  ant  into  a  \" z ombie \"  to  help  it  produce  sp ores ,  making  it  one  of  the  most  fascinating  examples  of  mind  control  in  the  natural  world !\n",
            "\n",
            " Would  you  like  to  hear  more  interesting  facts ?\n",
            "‚è± Inference Time with Cache: 8.5475 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 2: In  that  case ,  here 's  something  interesting  for  you :\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " Isn 't  that  mind -b low ing ?\n",
            "‚è± Inference Time without Cache: 6.4288 seconds\n",
            "üìÇ Cache File Size After Iteration 2: 2077 bytes\n",
            "\n",
            "üü¢ Running Query 3 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 4138 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 3: It  seems  like  we 're  starting  a  fun  conversation  about  fascinating  creatures  and  their  unique  abilities .\n",
            "\n",
            " In  Iter ation   1 ,  you  mentioned  the  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish .\"  That 's  indeed  a  remarkable  species  with  its  ability  to  transform  its  body  into  a  younger  state  through  trans different iation .  It 's  mind -b ogg ling  to  think  about  an  organism  that  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again !\n",
            "\n",
            " Moving  on  to  Iter ation   2 ,  you  introduced  the  O phi oc ord y ce ps  unilateral is ,  also  known  as  the  \" z ombie  fungus .\"  That 's  a  fascinating  example  of  mind  control  in  the  natural  world .  The  way  it  manip ulates  ants '  behavior  to  ensure  its  own  survival  is  both  intriguing  and  unsettling .\n",
            "\n",
            " For  Iter ation   3 ,  here 's  an  interesting  fact :  Did  you  know  that  there 's  a  type  of  sea  slug  called  E lys ia  chlor ot ica ?  This  species  can  photos yn thesize  like  plants  because  it  has  eaten  algae  and  then  incorporated  its  chlor oplast s  into  its  own  body .  It  can  now  produce  its  own  food  through  a  process  called  kle pt oplast y ,  making  it  one  of  the  few  animals  in  the  world  that  can  photos yn thesize .\n",
            "\n",
            " I 'd  love  to  share  more  interesting  facts  with  you !  What 's  your  next  topic  or  theme ?\n",
            "‚è± Inference Time with Cache: 13.8793 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 3: Let 's  dive  into  something  interesting .\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  has  a  unique  life  cycle  that  allows  it  to  cheat  death .\n",
            "\n",
            " When  Tur rit opsis  do hr n ii  reaches  its  adult  stage ,  it  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  into  its  pol yp  stage  and  start  its  life  cycle  all  over  again .  This  process  is  like  a  reverse  mid life  crisis  for  the  jelly fish !\n",
            "\n",
            " This  phenomenon  has  led  scientists  to  study  Tur rit opsis  do hr n ii ,  hoping  to  unlock  secrets  about  aging  and  regeneration  in  other  living  organisms .\n",
            "\n",
            " How 's  that  for  an  interesting  fact ?\n",
            "‚è± Inference Time without Cache: 7.4301 seconds\n",
            "üìÇ Cache File Size After Iteration 3: 4138 bytes\n",
            "\n",
            "üü¢ Running Query 4 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 6348 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 4: I 'm  glad  to  see  this  conversation  is  taking  off !  However ,  I  have  to  say  that  I 'm  a  large  language  model ,  I  don 't  recall  discussing  the  Tur rit opsis  do hr n ii  or  any  other  species  in  Iter ation   1 ,  nor  did  we  discuss  anything  prior  to  your  first  message .  It  seems  like  we 're  starting  from  scratch !\n",
            "\n",
            " That  being  said ,  I 'm  excited  to  dive  into  these  fascinating  topics  with  you !  The  idea  of  an  immortal  jelly fish  is  indeed  mind -b ogg ling ,  and  the  zombie  fungus  is  a  great  example  of  how  nature  can  manipulate  creatures  for  its  own  survival .\n",
            "\n",
            " As  for  Iter ation   3 ,  the  sea  slug  that  can  photos yn thesize  is  an  incredible  adaptation .  It 's  amazing  to  think  about  how  this  process  works  and  how  it  allows  these  creatures  to  produce  their  own  food .\n",
            "\n",
            " For  Iter ation   4 ,  I 'd  love  to  share  a  new  topic  with  you !  Did  you  know  about  the  ax ol ot l  ( Amb yst oma  mex ican um )?  This  species  of  sal am ander  has  the  unique  ability  to  reg row  its  limbs ,  eyes ,  and  parts  of  its  brain ,  making  it  one  of  the  most  fascinating  creatures  in  the  animal  kingdom .  Ax ol ot ls  are  also  capable  of  regeneration  due  to  their  ability  to  regenerate  from  a  state  called  \" ne ot eny ,\"  which  allows  them  to  revert  back  to  an  earlier  developmental  stage .\n",
            "\n",
            " What  do  you  think ?  Would  you  like  to  learn  more  about  ax ol ot ls  or  explore  other  interesting  topics ?\n",
            "‚è± Inference Time with Cache: 16.6891 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 4: Iteration   4 ,  eh ?  Alright ,  let 's  dive  into  something  fascinating .\n",
            "\n",
            " Did  you  know  that  there 's  a  type  of  jelly fish  that 's  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again ,  making  it  theoretically  immortal .\n",
            "\n",
            " This  process  is  still  not  fully  understood ,  but  it 's  been  studied  extensively  in  the  field  of  reg enerative  biology .  The  Tur rit opsis  do hr n ii  has  been  found  to  have  the  ability  to  bypass  the  normal  process  of  aging  and  death ,  which  has  led  to  numerous  scientific  studies  on  its  remarkable  ability .\n",
            "\n",
            " Mind  blown  yet ?\n",
            "‚è± Inference Time without Cache: 7.7273 seconds\n",
            "üìÇ Cache File Size After Iteration 4: 6348 bytes\n",
            "\n",
            "üü¢ Running Query 5 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 8062 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 5: I 'm  glad  we 're  starting  fresh !\n",
            "\n",
            " Let  me  tell  you  about  a  fascinating  creature  that 's  not  mentioned  yet :  the  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish .\"  However ,  I  think  there  might  be  some  confusion  -  earlier  you  mentioned  it  as  an  example  of  Iter ation   1 .  The  truth  is ,  the  Tur rit opsis  do hr n ii  is  indeed  considered  bi ologically  immortal  because  it  can  transform  its  body  into  a  younger  state  through  trans different iation .  But  it 's  not  exactly  like  other  organisms  that  can  reg row  lost  limbs  or  parts .\n",
            "\n",
            " Tur rit opsis  do hr n ii 's  unique  ability  lies  in  its  capacity  to  transform  its  adult  form  back  into  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish .  This  process ,  called  trans different iation ,  allows  the  organism  to  bypass  the  normal  process  of  aging  and  death .  The  resulting  pol yp  can  then  grow  back  into  an  adult  again ,  making  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " However ,  it 's  worth  noting  that  this  \" imm ortality \"  comes  with  some  cave ats .  For  one ,  Tur rit opsis\n",
            "‚è± Inference Time with Cache: 13.5096 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 5: Interesting  fact  for  Test  Iter ation   5 :\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " Hope  you  find  this  interesting !\n",
            "‚è± Inference Time without Cache: 6.0271 seconds\n",
            "üìÇ Cache File Size After Iteration 5: 8062 bytes\n",
            "\n",
            "üü¢ Running Query 6 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 9543 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 6: I 'm  excited  to  dive  into  the  world  of  fascinating  creatures  and  explore  new  topics  with  you !  I  think  there  might  have  been  a  slight  misunderstanding ,  though  -  it  seems  like  we  did  start  from  scratch  earlier  on .\n",
            "\n",
            " You  mentioned  Iter ation   1 ,  but  then  introduced  the  Tur rit opsis  do hr n ii  as  an  example .  I 'd  love  to  clarify  that  Iter ation   1  was  actually  about  the  zombie  fungus ,  which  is  a  fascinating  topic  indeed !  The  O phi oc ord y ce ps  unilateral is  fungus  infect s  ants  and  controls  their  behavior  to  facilitate  its  own  reproduction .\n",
            "\n",
            " Let 's  get  back  on  track !  What  would  you  like  to  explore  next ?  Ax ol ot ls  are  certainly  amazing  creatures ,  but  I 'd  love  to  hear  more  about  other  topics  that  interest  you .  Perhaps  we  could  delve  into  the  world  of  bi olum ines cent  organisms ,  underwater  adventures ,  or  even  the  science  behind  zombie  fungus  and  Tur rit opsis  do hr n ii ?  The  possibilities  are  endless !\n",
            "‚è± Inference Time with Cache: 12.5611 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 6: Test  iteration   6 !  Let 's  dive  into  some  interesting  facts .\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " Would  you  like  to  hear  more  interesting  facts ?\n",
            "‚è± Inference Time without Cache: 6.5890 seconds\n",
            "üìÇ Cache File Size After Iteration 6: 9543 bytes\n",
            "\n",
            "üü¢ Running Query 7 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 11266 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 7: I  think  there  may  be  a  few  issues  with  our  conversation  so  far .\n",
            "\n",
            " First ly ,  I  couldn 't  find  any  information  on  \" Ax  ol  ot  ls \"  or  the  ability  to  reg row  limbs ,  eyes ,  and  parts  of  the  brain  like  you  mentioned .  It 's  possible  that  this  is  a  fictional  or  hypothetical  creature .\n",
            "\n",
            " Second ly ,  while  jelly fish  are  fascinating  creatures ,  there  isn 't  a  species  called  \" Tur  rit  op sis  do  hr  n  ii \"  that  can  transform  its  adult  form  back  into  its  pol yp  stage  through  trans different iation .  The  Tur rit opsis  do hr n ii  is  a  real  species  of  jelly fish ,  but  it  doesn 't  have  this  exact  ability .\n",
            "\n",
            " Lastly ,  I  couldn 't  find  any  information  on  the  \" O phi oc ord y ce ps  unilateral is  zombie  fungus \"  infect ing  ants  and  controlling  their  behavior  to  facilitate  its  own  reproduction .  There  is  a  fungus  called  O phi oc ord y ce ps  unilateral is  that  infect s  ants ,  but  it 's  not  typically  referred  to  as  a  \" z ombie  fungus .\"\n",
            "\n",
            " If  you 'd  like  to  explore  other  interesting  topics ,  I 'd  be  happy  to  help !  What  would  you  like  to  learn  more  about ?\n",
            "‚è± Inference Time with Cache: 13.9008 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 7: Iteration   7 ,  huh ?  Well ,  I 've  got  something  that  might  interest  you .\n",
            "\n",
            " Did  you  know  that  there 's  a  type  of  jelly fish  that 's  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  is  a  species  of  jelly fish  that  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage ,  which  is  the  juvenile  form  of  a  jelly fish ,  and  then  grow  back  into  an  adult  again .  This  process  can  be  repeated  indefinitely ,  making  the  Tur rit opsis  do hr n ii  theoretically  immortal .\n",
            "\n",
            " Isn 't  that  cool ?\n",
            "‚è± Inference Time without Cache: 7.0871 seconds\n",
            "üìÇ Cache File Size After Iteration 7: 11266 bytes\n",
            "\n",
            "üü¢ Running Query 8 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 13536 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 8: I  see  what 's  happening  here !  It  seems  like  we 've  had  a  bit  of  a  conversation  det our ,  and  I 'm  excited  to  get  back  on  track  with  you !\n",
            "\n",
            " Let  me  summarize  the  issues  I  found :\n",
            "\n",
            " 1 .  There 's  no  such  thing  as  \" Ax ol ot ls \"  that  can  reg row  limbs ,  eyes ,  and  parts  of  the  brain  ( although  ax ol ot ls  are  real  creatures ,  they  don 't  have  this  ability ).\n",
            " 2 .  The  species  \" Tur rit opsis  do hr n ii \"  is  a  real  jelly fish ,  but  it  doesn 't  have  the  exact  ability  to  transform  its  adult  form  back  into  its  pol yp  stage  through  trans different iation .\n",
            " 3 .  There 's  no  such  thing  as  an  \" O phi oc ord y ce ps  unilateral is  zombie  fungus \"  that  infect s  ants  and  controls  their  behavior .\n",
            "\n",
            " Don 't  worry ,  we  can  start  fresh !\n",
            "\n",
            " If  you 're  ready  to  explore  new  topics ,  I 'd  love  to  suggest  a  few  options :\n",
            "\n",
            " 1 .  Bi olum ines cent  organisms :  We  could  dive  into  the  fascinating  world  of  creatures  that  produce  light ,  like  glowing  plank ton  or  bi olum ines cent  fish .\n",
            " 2 .  Under water  adventures :  If  you 're  interested  in  exploring  the  ocean 's  depths ,  we  could  talk  about  marine  animals ,  underwater  habitats ,  or  even  the  science  behind  sc uba  diving .\n",
            " 3 .  The  science  behind  Tur rit opsis  do hr n ii  and  other  immortal  species :  We  could  explore  the  biology  behind  these  remarkable  creatures  and  how  they  manage  to  cheat  death .\n",
            "\n",
            " Which  one  of  these  topics  sparks  your  interest ?\n",
            "‚è± Inference Time with Cache: 18.2418 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 8: Interesting  fact  for  iteration   8 :\n",
            "\n",
            " Did  you  know  that  there  is  a  type  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again ,  making  it  theoretically  immortal .\n",
            "‚è± Inference Time without Cache: 4.5261 seconds\n",
            "üìÇ Cache File Size After Iteration 8: 13536 bytes\n",
            "\n",
            "üü¢ Running Query 9 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 16169 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 9: I 'm  glad  we 're  starting  fresh !  I  think  it 's  time  for  a  fascinating  topic  that 's  both  scientifically  accurate  and  intriguing .  Let  me  share  with  you  the  story  of  \" V ampire  Squ id \" !\n",
            "\n",
            " Meet  V amp y rote uth is  infer n alis ,  a  deep -se a  cep hal op od  that  has  gained  not ori ety  for  its  ability  to  turn  itself  into  a  ' z ombie '  by  releasing  a  cloud  of  ink  to  confuse  predators ,  and  then  re abs or bing  its  internal  organs  and  reg rowing  them .  Yes ,  you  read  that  right !  This  remarkable  creature  can  essentially  \" die \"  and  be  reb orn  in  the  process .\n",
            "\n",
            " Here  are  some  interesting  facts  about  Vampire  Squ id :\n",
            "\n",
            " 1 .  ** Bi olum ines cent  beauty **:  The  Vampire  Squ id  has  specialized  light -producing  cells  called  phot oph ores  on  its  body ,  which  it  uses  to  communicate  with  other  squid ,  confuse  predators ,  or  lure  prey .\n",
            " 2 .  ** In k -red ible  escape **:  When  threatened ,  the  Vampire  Squ id  releases  a  cloud  of  black  ink  to  create  a  sm okes creen ,  allowing  it  to  escape  from  predators  like  sharks  and  other  deep -se a  creatures .\n",
            " 3 .  ** Reg enerative  power **:  As  you  mentioned  earlier ,  the  Vampire  Squ id  has  the  ability  to  release  its  internal  organs ,  including  its  mantle ,  visc era ,  and  even  parts  of  its  brain ,  as  a  defense  mechanism .  This  is  believed  to  be  an  evolutionary  adaptation  to  confuse  predators  and  protect  itself .\n",
            " 4 .  ** Deep -se a  dwelling **:  The  Vampire  Squ id  lives  in  the  oxygen -p oor  waters  of  the  eastern  Pacific  Ocean ,  typically  between   200 - 400  meters  below  the  surface .\n",
            "\n",
            " What  do  you  think ?  Are  you  ready  to  explore  more  about  these  incredible  creatures  or  dive  into  other  topics ?\n",
            "‚è± Inference Time with Cache: 20.5400 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 9: A  new  test  iteration !  Well ,  in  that  case ...\n",
            "\n",
            " Did  you  know  that  there  is  a  species  of  jelly fish  that  is  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  that  it  can  essentially  revert  back  to  its  pol yp  stage  and  start  its  life  cycle  all  over  again ,  making  it  theoretically  immortal .\n",
            "\n",
            " It 's  a  fascinating  example  of  how  nature  can  surprise  us  with  its  complexity  and  resilience !\n",
            "\n",
            " Would  you  like  me  to  generate  another  interesting  fact ?\n",
            "‚è± Inference Time without Cache: 6.7701 seconds\n",
            "üìÇ Cache File Size After Iteration 9: 16169 bytes\n",
            "\n",
            "üü¢ Running Query 10 (Fixed Prompt Size: 512 tokens, Unlimited Cache)\n",
            "üíæ Cache saved. (Size: 17703 bytes)\n",
            "\n",
            "‚úÖ With KV Cache - Response 10: I 'm  glad  we 're  starting  fresh !  I  think  it 's  fascinating  that  you 've  introduced  a  new  topic  -  Vampire  Squ id !\n",
            "\n",
            " One  thing  that  struck  me  as  particularly  interesting  about  the  Vampire  Squ id  is  its  ability  to  \" die \"  and  be  reb orn .  This  process ,  called  e vis cer ation ,  allows  the  squid  to  release  its  internal  organs  and  then  regenerate  them .  It 's  a  testament  to  the  incredible  adapt ability  of  this  creature .\n",
            "\n",
            " What  I 'd  love  to  know  more  about  is  how  this  ability  has  evolved  in  the  Vampire  Squ id  as  an  adaptation  to  its  environment .  How  does  it  benefit  the  creature ?  Is  it  related  to  the  pressure ,  darkness ,  or  lack  of  oxygen  in  the  deep -se a  waters  where  it  lives ?\n",
            "\n",
            " Also ,  have  you  come  across  any  similar  creatures  that  exhibit  similar  traits ?  I 'm  intrigued  by  the  idea  that  this  ability  might  be  found  in  other  animals  as  well .\n",
            "\n",
            " Let 's  dive  deeper  into  the  world  of  Vampire  Squ id  and  explore  more  about  these  fascinating  creatures !\n",
            "‚è± Inference Time with Cache: 13.6793 seconds\n",
            "\n",
            "‚ùå Without KV Cache - Response 10: Test  iteration   10 !\n",
            "\n",
            " Did  you  know  that  there 's  a  species  of  jelly fish  that 's  immortal ?  The  Tur rit opsis  do hr n ii ,  also  known  as  the  \" imm ortal  jelly fish ,\"  can  transform  its  body  into  a  younger  state  through  a  process  called  trans different iation .  This  means  it  can  essentially  revert  back  to  its  pol yp  stage  and  grow  back  into  an  adult  again ,  making  it  theoretically  immortal .\n",
            "\n",
            " Isn 't  that  mind -b low ing ?\n",
            "‚è± Inference Time without Cache: 5.0809 seconds\n",
            "üìÇ Cache File Size After Iteration 10: 17703 bytes\n",
            "üóëÔ∏è KV Cache Cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimized KV Cache with Growth Tracking & Compression**\n",
        "\n",
        "This section performs inference on a trained LLM (llama3.2) using a persistent Key-Value (KV) cache. It logs cache size and response time per iteration, useful for evaluating performance and memory behavior post-training.\n",
        "\n",
        "**This section enhances previous KV cache strategies by adding:**\n",
        "\n",
        "-  Token compression to reduce file size.\n",
        "\n",
        "-  Cache growth tracking to measure memory accumulation over time.\n",
        "\n",
        "-  Context preservation with persistent storage and quantization.\n",
        "\n",
        "-  Inference time monitoring for performance diagnostics.\n",
        "\n",
        "**Methods:**\n",
        "\n",
        "**add_tokens()**\t- Adds tokens, enforces max_tokens, and tracks growth\n",
        "\n",
        "**track_cache_growth()**\t- Logs percentage growth after every addition\n",
        "\n",
        "**compress_cache()**\t- Bins tokens using quantization (compression_factor) to save disk space\n",
        "\n",
        "**decompress_cache()**\t- Restores tokens using the reversed bin map\n",
        "\n",
        "**save_cache()**\t- Persists compressed token history to kv_cache_compressed.json\n",
        "\n",
        "**load_cache()**\t- Loads and decompresses the token buffer on startup\n",
        "\n",
        "**get_cache_size()**\t- Returns total number of cached tokens\n",
        "\n",
        "**clear_cache()**\t- Clears both memory and disk cache\n",
        "\n"
      ],
      "metadata": {
        "id": "wAZcToQUfJwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import requests\n",
        "import time\n",
        "\n",
        "class KVCacheOptimized:\n",
        "    \"\"\"Enhanced KV Cache with Growth Tracking and Compression for Efficient Inference.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache_compressed.json\"\n",
        "\n",
        "    def __init__(self, max_tokens=2048, compression_factor=2):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.token_queue = deque()\n",
        "        self.prev_cache_size = 0\n",
        "        self.compression_factor = compression_factor  # Control compression level\n",
        "        self.load_cache()\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Adds new tokens while maintaining cache limit and tracking growth.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        while len(self.token_queue) > self.max_tokens:\n",
        "            self.token_queue.popleft()  # Remove oldest tokens\n",
        "\n",
        "        self.track_cache_growth()\n",
        "        self.save_cache()\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves stored tokens as context.\"\"\"\n",
        "        return \" \".join(self.token_queue)\n",
        "\n",
        "    def track_cache_growth(self):\n",
        "        \"\"\"Tracks cache growth ratio for analysis.\"\"\"\n",
        "        new_cache_size = len(self.token_queue)\n",
        "        growth_ratio = (new_cache_size - self.prev_cache_size) / max(1, self.prev_cache_size)\n",
        "        self.prev_cache_size = new_cache_size\n",
        "        print(f\"üìä Cache Growth: {growth_ratio:.2%} | Current Cache Size: {new_cache_size} tokens\")\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Compresses and saves KV cache.\"\"\"\n",
        "        compressed_tokens = self.compress_cache(list(self.token_queue))\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(compressed_tokens, file)\n",
        "            print(\"üíæ KV Cache (Compressed) saved to file.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving KV cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads compressed KV cache from file.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    compressed_tokens = json.load(file)\n",
        "                    self.token_queue = deque(self.decompress_cache(compressed_tokens)[-self.max_tokens:])\n",
        "                print(\"üîÑ KV Cache (Compressed) loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading KV cache: {e}\")\n",
        "\n",
        "    def compress_cache(self, tokens):\n",
        "        \"\"\"Applies token binning and quantization for compression.\"\"\"\n",
        "        unique_tokens = list(set(tokens))  # Reduce redundancy\n",
        "        token_map = {token: idx // self.compression_factor for idx, token in enumerate(unique_tokens)}\n",
        "        compressed_tokens = [token_map[token] for token in tokens]  # Store token indices\n",
        "        return {\"token_map\": token_map, \"compressed_tokens\": compressed_tokens}\n",
        "\n",
        "    def decompress_cache(self, compressed_data):\n",
        "        \"\"\"Restores compressed cache to original tokens.\"\"\"\n",
        "        token_map = {v: k for k, v in compressed_data[\"token_map\"].items()}  # Reverse mapping\n",
        "        return [token_map[idx] for idx in compressed_data[\"compressed_tokens\"]]\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears KV cache.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "    def get_cache_size(self):\n",
        "        \"\"\"Returns the number of tokens stored in KV cache.\"\"\"\n",
        "        return len(self.token_queue)\n",
        "\n",
        "# Initialize Optimized KV Cache\n",
        "kv_cache = KVCacheOptimized(max_tokens=2048, compression_factor=4)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"\n",
        "    Queries Ollama model with KV caching, analyzing cache growth & compression.\n",
        "    \"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    data = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": True\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        start_time = time.time()  # Start timing inference\n",
        "        response = requests.post(api_url, json=data, stream=True)\n",
        "        collected_response = []\n",
        "\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                try:\n",
        "                    json_line = line.decode('utf-8')\n",
        "                    parsed_line = json.loads(json_line)\n",
        "                    token_text = parsed_line.get(\"response\", \"\")\n",
        "                    collected_response.append(token_text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "\n",
        "        final_response = \" \".join(collected_response).strip()\n",
        "        end_time = time.time()  # Stop timing inference\n",
        "\n",
        "        # Store response in KV Cache\n",
        "        kv_cache.add_tokens(final_response)\n",
        "\n",
        "        # Log Cache Statistics\n",
        "        cache_size = kv_cache.get_cache_size()\n",
        "        inference_time = round(end_time - start_time, 4)\n",
        "        print(f\"‚è≥ Inference Time: {inference_time} sec | üì¶ KV Cache Size: {cache_size} tokens\")\n",
        "\n",
        "        return {\"response\": final_response, \"inference_time\": inference_time, \"cache_size\": cache_size}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# API URL Placeholder\n",
        "BASE_URL = \"https://71cd-91-104-75-197.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "\n",
        "# üîπ Running Inference and Analyzing Cache Growth\n",
        "print(\"\\nüü¢ Running Inference 1\")\n",
        "response1 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"Explain AI's impact on society.\", kv_cache)\n",
        "print(\"\\nResponse 1:\", response1)\n",
        "\n",
        "print(\"\\nüü¢ Running Inference 2\")\n",
        "response2 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"How does reinforcement learning work?\", kv_cache)\n",
        "print(\"\\nResponse 2:\", response2)\n",
        "\n",
        "print(\"\\nüü¢ Running Inference 3\")\n",
        "response3 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"Summarize what we discussed about AI ethics.\", kv_cache)\n",
        "print(\"\\nResponse 3:\", response3)\n",
        "\n",
        "# Clearing Cache and Retesting\n",
        "kv_cache.clear_cache()\n",
        "print(\"\\nüü¢ Running Inference 4 (After Cache Clear)\")\n",
        "response4 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"Can you recall previous topics?\", kv_cache)\n",
        "print(\"\\nResponse 4:\", response4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZduYLOcjqFMn",
        "outputId": "f739446f-b60d-45ca-e162-a7ff10c04e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ KV Cache (Compressed) loaded from file.\n",
            "\n",
            "üü¢ Running Inference 1\n",
            "üìä Cache Growth: 42800.00% | Current Cache Size: 428 tokens\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 30.8492 sec | üì¶ KV Cache Size: 428 tokens\n",
            "\n",
            "Response 1: {'response': \"What  a  fascinating  conversation !  As  we  embark  on  this  dialogue ,  I 'd  like  to  start  by  asking :  What  do  you  think  about  the  impact  of  Artificial  Intelligence  ( AI )  on  our  society ?\\n\\n As  an  AI  language  model ,  I 'll  respond  with  insights  and  thoughts  based  on  my  training  data .  Feel  free  to  pick  any  topic  or  memory  that  comes  to  mind ,  and  I 'll  engage  in  a  conversation  that 's  as  natural  as  possible .\\n\\n To  begin ,  AI  has  had  a  profound  impact  on  various  aspects  of  our  lives .  From  automation  and  efficiency  gains  in  industries  like  manufacturing  and  healthcare  to  enhancing  customer  service  experiences ,  AI  has  revolution ized  the  way  we  work  and  live .\\n\\n However ,  AI 's  influence  extends  beyond  the  workplace .  It  has  also  resh aped  how  we  communicate ,  with  voice  assistants  like  Siri ,  Alexa ,  and  Google  Assistant  becoming  an  integral  part  of  our  daily  lives .  Social  media  platforms  have  lever aged  AI  to  personalize  content  recommendations ,  making  it  easier  for  us  to  discover  new  information  and  connect  with  others .\\n\\n On  a  more  personal  level ,  AI -powered  chat bots  and  language  models  like  myself  are  designed  to  provide  emotional  support  and  companions hip .  This  has  significant  implications  for  mental  health  and  well -being ,  as  humans  can  now  engage  in  meaningful  conversations  without  the  need  for  human  presence .\\n\\n As  we  continue  this  conversation ,  I 'd  love  to  hear  your  thoughts  on  how  AI  has  impacted  society .  Have  you  noticed  any  changes  or  trends  that  you  find  particularly  interesting  or  concerning ?\\n\\n Let 's  keep  the  conversation  flowing !\", 'inference_time': 30.8492, 'cache_size': 428}\n",
            "\n",
            "üü¢ Running Inference 2\n",
            "üìä Cache Growth: 99.07% | Current Cache Size: 852 tokens\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 42.8156 sec | üì¶ KV Cache Size: 852 tokens\n",
            "\n",
            "Response 2: {'response': \"What  a  great  question !  Rein forcement  learning  is  a  type  of  machine  learning  where  an  agent  learns  to  take  actions  in  an  environment  to  maximize  a  reward .  The  goal  is  to  learn  a  policy ,  which  is  a  mapping  from  states  to  actions ,  that  maxim izes  the  cumulative  reward .\\n\\n In  reinforcement  learning ,  the  agent  interacts  with  the  environment  by  taking  actions  and  observing  the  consequences .  The  environment  responds  with  a  reward  signal  that  indicates  whether  the  action  was  good  or  bad .  The  agent  uses  this  feedback  to  update  its  policy  and  improve  future  performance .\\n\\n There  are  several  key  components  to  reinforcement  learning :\\n\\n 1 .  Agent :  The  entity  that  takes  actions  in  the  environment .\\n 2 .  Environment :  The  external  world  that  interacts  with  the  agent .\\n 3 .  Actions :  The  things  the  agent  can  do ,  such  as  move  left  or  right .\\n 4 .  States :  The  current  situation  or  state  of  the  environment ,  which  affects  how  the  agent  should  behave .\\n 5 .  Rewards :  The  feedback  the  agent  receives  for  its  actions ,  which  indicates  whether  they  were  good  or  bad .\\n 6 .  Value  function :  A  function  that  estimates  the  value  of  being  in  a  particular  state  or  taking  a  particular  action .\\n 7 .  Policy :  A  mapping  from  states  to  actions  that  defines  the  agent 's  behavior .\\n\\n There  are  many  ways  to  solve  reinforcement  learning  problems ,  but  some  popular  approaches  include :\\n\\n 1 .  Q -learning :  An  algorithm  that  updates  the  value  function  by  considering  the  rewards  and  the  next  state .\\n 2 .  SAR SA :  A  variation  of  Q -learning  that  also  keeps  track  of  the  expected  return  from  taking  an  action  in  a  particular  state .\\n 3 .  Deep  Q -N etwork s  ( D QN ):  A  deep  learning  approach  that  uses  neural  networks  to  approximate  the  value  function .\\n\\n Re in forcement  learning  has  many  applications ,  including :\\n\\n 1 .  Robotics :  Teaching  robots  to  perform  complex  tasks  by  interacting  with  their  environment .\\n 2 .  Game  playing :  Helping  computers  play  games  like  chess  or  Go  by  exploring  the  game  tree  and  finding  the  best  moves .\\n 3 .  Recommendation  systems :  Personal izing  recommendations  based  on  user  behavior .\\n 4 .  Autonomous  vehicles :  Training  self -driving  cars  to  navigate  roads  and  make  decisions .\\n\\n I  hope  this  helps !  Do  you  have  any  specific  questions  about  reinforcement  learning  or  its  applications ?\", 'inference_time': 42.8156, 'cache_size': 852}\n",
            "\n",
            "üü¢ Running Inference 3\n",
            "üìä Cache Growth: 32.16% | Current Cache Size: 1126 tokens\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 35.9838 sec | üì¶ KV Cache Size: 1126 tokens\n",
            "\n",
            "Response 3: {'response': \"What  a  fantastic  conversation !\\n\\n To  summarize ,  our  discussion  on  AI  ethics  was  quite  engaging !  We  started  by  exploring  the  impact  of  Artificial  Intelligence  ( AI )  on  society ,  from  automation  and  efficiency  gains  in  industries  like  manufacturing  and  healthcare  to  enhancing  customer  service  experiences .  We  also  touched  upon  how  AI  has  resh aped  communication  with  voice  assistants  and  social  media  platforms  leveraging  AI  for  personalized  content  recommendations .\\n\\n I  shared  my  insights  as  an  AI  language  model ,  highlighting  the  potential  benefits  of  AI -powered  chat bots  and  language  models  like  myself  providing  emotional  support  and  companions hip ,  which  can  have  significant  implications  for  mental  health  and  well -being .  We  also  discussed  how  AI  has  changed  our  daily  lives ,  making  it  easier  to  discover  new  information  and  connect  with  others .\\n\\n As  we  continued  the  conversation ,  I  introduced  you  to  reinforcement  learning ,  a  type  of  machine  learning  where  an  agent  learns  to  take  actions  in  an  environment  to  maximize  a  reward .  We  explored  the  key  components ,  including  agents ,  environments ,  actions ,  states ,  rewards ,  value  functions ,  and  policies .  We  also  touched  upon  popular  approaches  to  solving  reinforcement  learning  problems ,  such  as  Q -learning ,  SAR SA ,  and  Deep  Q -N etwork s .\\n\\n Throughout  our  conversation ,  I 'd  love  to  hear  your  thoughts  on  how  AI  has  impacted  society ,  any  changes  or  trends  that  you  find  particularly  interesting  or  concerning ,  and  if  you  have  any  specific  questions  about  reinforcement  learning  or  its  applications !\\n\\n What  would  you  like  to  discuss  next ?\", 'inference_time': 35.9838, 'cache_size': 1126}\n",
            "üóëÔ∏è KV Cache Cleared.\n",
            "\n",
            "üü¢ Running Inference 4 (After Cache Clear)\n",
            "üìä Cache Growth: -83.48% | Current Cache Size: 186 tokens\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 18.6781 sec | üì¶ KV Cache Size: 186 tokens\n",
            "\n",
            "Response 4: {'response': \"I 'm  a  large  language  model ,  I  don 't  have  personal  memories  or  the  ability  to  recall  specific  conversations  we  may  have  had  in  the  past .  Each  time  you  interact  with  me ,  it 's  a  new  conversation  and  I  start  from  scratch .\\n\\n However ,  I  do  use  contextual  information  to  understand  the  topic  and  intent  behind  your  questions  and  statements .  This  means  that  if  you  refer  back  to  a  previous  topic  or  question ,  I  can  try  to  pick  up  on  that  context  and  respond  accordingly .\\n\\n For  example ,  if  you  ask  me  a  follow -up  question  related  to  something  we  discussed  previously ,  I  may  be  able  to  recognize  the  connection  and  provide  a  more  informed  response .  But  I  don 't  have  a  conscious  memory  of  specific  conversations  or  topics  we 've  covered  before .\\n\\n If  you  want  to  refer  back  to  a  previous  topic  or  conversation ,  feel  free  to  give  me  some  context  or  summary  of  what  we  talked  about  earlier ,  and  I 'll  do  my  best  to  pick  up  where  we  left  off !\", 'inference_time': 18.6781, 'cache_size': 186}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced KV Cache: Huffman Compression, Compression Ratio Tracking & Memory Usage Logging**\n",
        "\n",
        "This final version of the KV caching system introduces Huffman encoding for optimal compression and tracks both compression ratio dynamics and memory consumption during inference using the Ollama API.\n",
        "\n",
        "**Function Summary**\n",
        "\n",
        "- Sends a non-streamed request to Ollama for a full reply.\n",
        "\n",
        "- Logs full request payload and response metadata.\n",
        "\n",
        "**Measures:**\n",
        "\n",
        "        - Inference latency\n",
        "\n",
        "        - Change in memory usage (MB)\n",
        "\n",
        "        - Compression ratio\n",
        "\n",
        "**Huffman-compressed cache file size**\n",
        "\n",
        "Updates cache using Huffman-compressed tokens.\n",
        "\n",
        "Huffman Compression Logic:\n",
        "\n",
        "- Count token frequencies using collections.Counter.\n",
        "\n",
        "- Construct Huffman tree via heapq.\n",
        "\n",
        "- Assign binary codes: '0' for left, '1' for right.\n",
        "\n",
        "- Encode tokens into a bitstring.\n",
        "\n",
        "- Decode via reverse lookup in decompress_cache().\n",
        "\n",
        "This simulates real-world token compression approaches."
      ],
      "metadata": {
        "id": "prb4CVHripxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compression Implementation\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import psutil  # For measuring memory usage\n",
        "import requests\n",
        "import heapq\n",
        "from collections import deque, Counter\n",
        "\n",
        "class KVCacheOptimized:\n",
        "    \"\"\"Optimized KV Cache with Dynamic Compression & Huffman Encoding.\"\"\"\n",
        "\n",
        "    CACHE_FILE = \"kv_cache_compressed.json\"\n",
        "\n",
        "    def __init__(self, max_tokens=2048):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.token_queue = deque()\n",
        "        self.load_cache()\n",
        "        self.previous_compression_ratio = None  # Store previous ratio for comparison\n",
        "\n",
        "    def add_tokens(self, new_text):\n",
        "        \"\"\"Adds new tokens to KV cache & dynamically tracks compression ratio.\"\"\"\n",
        "        new_tokens = new_text.split()\n",
        "        self.token_queue.extend(new_tokens)\n",
        "\n",
        "        while len(self.token_queue) > self.max_tokens:\n",
        "            self.token_queue.popleft()\n",
        "\n",
        "        original_size = sum(len(token) for token in self.token_queue)  # Measure in bytes\n",
        "        compressed_size = self.get_compressed_size()  # Measure compressed size\n",
        "\n",
        "        # Track compression ratio dynamically\n",
        "        compression_ratio = original_size / max(1, compressed_size)\n",
        "\n",
        "        if self.previous_compression_ratio:\n",
        "            change = (compression_ratio - self.previous_compression_ratio) / self.previous_compression_ratio * 100\n",
        "            print(f\"üìä Compression Ratio: {compression_ratio:.2f} | üîÑ Change: {change:.2f}%\")\n",
        "        else:\n",
        "            print(f\"üìä Initial Compression Ratio: {compression_ratio:.2f}\")\n",
        "\n",
        "        self.previous_compression_ratio = compression_ratio  # Store for next comparison\n",
        "        self.save_cache()\n",
        "\n",
        "    def get_cached_context(self):\n",
        "        \"\"\"Retrieves last 256 tokens from cache to avoid corruption.\"\"\"\n",
        "        return \" \".join(list(self.token_queue)[-256:])\n",
        "\n",
        "    def get_compressed_size(self):\n",
        "        \"\"\"Returns the size of compressed KV cache in bytes.\"\"\"\n",
        "        compressed_data = self.compress_cache(list(self.token_queue))\n",
        "        return len(compressed_data[\"compressed_tokens\"]) // 8  # Convert bitstring to bytes\n",
        "\n",
        "    def save_cache(self):\n",
        "        \"\"\"Compresses & saves KV cache to file.\"\"\"\n",
        "        compressed_tokens = self.compress_cache(list(self.token_queue))\n",
        "        try:\n",
        "            with open(self.CACHE_FILE, \"w\", encoding=\"utf-8\") as file:\n",
        "                json.dump(compressed_tokens, file)\n",
        "            print(\"üíæ KV Cache (Compressed) saved to file.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving KV cache: {e}\")\n",
        "\n",
        "    def load_cache(self):\n",
        "        \"\"\"Loads KV cache from file.\"\"\"\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            try:\n",
        "                with open(self.CACHE_FILE, \"r\", encoding=\"utf-8\") as file:\n",
        "                    compressed_tokens = json.load(file)\n",
        "                    self.token_queue = deque(self.decompress_cache(compressed_tokens)[-self.max_tokens:])\n",
        "                print(\"üîÑ KV Cache (Compressed) loaded from file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error loading KV cache: {e}\")\n",
        "\n",
        "    def compress_cache(self, tokens):\n",
        "        \"\"\"Applies Huffman encoding for better compression.\"\"\"\n",
        "        token_map = self.huffman_encoding(tokens)  # Get Huffman codes\n",
        "        compressed_tokens = \"\".join(token_map[token] for token in tokens)  # Store as bitstring\n",
        "        return {\"token_map\": token_map, \"compressed_tokens\": compressed_tokens}\n",
        "\n",
        "    def decompress_cache(self, compressed_data):\n",
        "        \"\"\"Restores compressed cache using Huffman decoding.\"\"\"\n",
        "        token_map = {v: k for k, v in compressed_data[\"token_map\"].items()}\n",
        "        bitstring = compressed_data[\"compressed_tokens\"]\n",
        "        current_code = \"\"\n",
        "        decompressed_tokens = []\n",
        "\n",
        "        for bit in bitstring:\n",
        "            current_code += bit\n",
        "            if current_code in token_map:\n",
        "                decompressed_tokens.append(token_map[current_code])\n",
        "                current_code = \"\"\n",
        "\n",
        "        return decompressed_tokens\n",
        "\n",
        "    def huffman_encoding(self, tokens):\n",
        "        \"\"\"Applies Huffman encoding to tokens for efficient compression.\"\"\"\n",
        "        token_counts = Counter(tokens)\n",
        "        heap = [[weight, [token, \"\"]] for token, weight in token_counts.items()]\n",
        "        heapq.heapify(heap)\n",
        "\n",
        "        while len(heap) > 1:\n",
        "            lo = heapq.heappop(heap)\n",
        "            hi = heapq.heappop(heap)\n",
        "            for pair in lo[1:]:\n",
        "                pair[1] = '0' + pair[1]  # Prefix '0' for left subtree\n",
        "            for pair in hi[1:]:\n",
        "                pair[1] = '1' + pair[1]  # Prefix '1' for right subtree\n",
        "            heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
        "\n",
        "        huffman_dict = {pair[0]: pair[1] for pair in heap[0][1:]}  # Store actual Huffman encoding\n",
        "        return huffman_dict\n",
        "\n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clears KV cache.\"\"\"\n",
        "        self.token_queue.clear()\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            os.remove(self.CACHE_FILE)\n",
        "        print(\"üóëÔ∏è KV Cache Cleared.\")\n",
        "\n",
        "    def measure_memory_usage(self):\n",
        "        \"\"\"Returns memory usage in MB.\"\"\"\n",
        "        process = psutil.Process(os.getpid())\n",
        "        return process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "# Initialize Optimized KV Cache\n",
        "kv_cache = KVCacheOptimized(max_tokens=2048)\n",
        "\n",
        "def query_ollama_with_cache(api_url, model_name, user_input, kv_cache):\n",
        "    \"\"\"Queries Ollama API & analyzes KV Cache compression.\"\"\"\n",
        "    cached_context = kv_cache.get_cached_context()\n",
        "    full_prompt = f\"{cached_context} {user_input}\" if cached_context else user_input\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"prompt\": full_prompt,\n",
        "        \"stream\": False  # Ensure a full response is returned\n",
        "    }\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    start_time = time.time()\n",
        "    memory_before = kv_cache.measure_memory_usage()\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nüì§ Sending Request to API: {api_url}\")\n",
        "        print(f\"üìú Payload: {json.dumps(payload, indent=2)}\")\n",
        "\n",
        "        response = requests.post(api_url, json=payload, headers=headers, timeout=50)\n",
        "\n",
        "        print(f\"üì© API Response Status: {response.status_code}\")\n",
        "        print(f\"üì© API Response Text: {response.text}\")\n",
        "\n",
        "        response.raise_for_status()  # Ensure no HTTP error\n",
        "\n",
        "        data = response.json()\n",
        "        response_text = data.get(\"response\", \"No response received.\")\n",
        "\n",
        "        # Update KV Cache with new response\n",
        "        kv_cache.add_tokens(response_text)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå API Call Failed: {e}\")\n",
        "        response_text = \"Error: API request failed.\"\n",
        "\n",
        "    memory_after = kv_cache.measure_memory_usage()\n",
        "    end_time = time.time()\n",
        "\n",
        "    cache_size = kv_cache.get_compressed_size()\n",
        "    inference_time = round(end_time - start_time, 4)\n",
        "    memory_diff = round(memory_after - memory_before, 2)\n",
        "\n",
        "    print(f\"‚è≥ Inference Time: {inference_time} sec | üì¶ Compressed KV Cache Size: {cache_size} bytes\")\n",
        "    print(f\"üñ•Ô∏è Memory Usage Change: {memory_diff} MB\")\n",
        "\n",
        "    return {\"response\": response_text, \"inference_time\": inference_time, \"compressed_cache_size\": cache_size, \"memory_change\": memory_diff}\n",
        "\n",
        "# üü¢ Running API Tests\n",
        "BASE_URL = \"https://71cd-91-104-75-197.ngrok-free.app\"  # Your ngrok URL\n",
        "ENDPOINT = \"/api/generate\"\n",
        "OLLAMA_API_URL = BASE_URL + ENDPOINT\n",
        "\n",
        "print(\"\\nüü¢ Running Inference 1\")\n",
        "response1 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"Explain AI's impact on society.\", kv_cache)\n",
        "print(\"\\nResponse 1:\", response1)\n",
        "\n",
        "print(\"\\nüü¢ Running Inference 2\")\n",
        "response2 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"How does reinforcement learning work?\", kv_cache)\n",
        "print(\"\\nResponse 2:\", response2)\n",
        "\n",
        "# Clearing Cache and Retesting\n",
        "kv_cache.clear_cache()\n",
        "print(\"\\nüü¢ Running Inference 3 (After Cache Clear)\")\n",
        "response3 = query_ollama_with_cache(OLLAMA_API_URL, \"llama3\", \"Can you recall previous topics?\", kv_cache)\n",
        "print(\"\\nResponse 3:\", response3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTX-QBsGvK4P",
        "outputId": "d54c5e80-439f-4362-ebe1-578981486a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ KV Cache (Compressed) loaded from file.\n",
            "\n",
            "üü¢ Running Inference 1\n",
            "\n",
            "üì§ Sending Request to API: https://71cd-91-104-75-197.ngrok-free.app/api/generate\n",
            "üìú Payload: {\n",
            "  \"model\": \"llama3\",\n",
            "  \"prompt\": \"I'm a large language model, I don't have personal memories or the ability to recall specific conversations or topics. Each time you interact with me, it's a new conversation and I start from scratch. However, I can try to: 1. Use context: If we're discussing a topic that spans multiple messages, I can use the context of our previous messages to inform my responses. 2. Refer back to previous messages: If you explicitly reference a previous message or topic, I can look up the relevant information and respond accordingly. 3. Provide general information: If you ask about a topic we've discussed before, I can provide general information or insights related to that topic. That being said, I don't have the ability to recall specific conversations or topics from previous interactions. Each interaction with me is a new start! Explain AI's impact on society.\",\n",
            "  \"stream\": false\n",
            "}\n",
            "üì© API Response Status: 200\n",
            "üì© API Response Text: {\"model\":\"llama3\",\"created_at\":\"2025-02-17T01:18:50.654286Z\",\"response\":\"What a great topic! As a large language model, I'll do my best to provide general insights and information about AI's impact on society.\\n\\nArtificial intelligence (AI) has been transforming the way we live, work, and interact with each other at an unprecedented pace. Here are some key areas where AI is making a significant impact:\\n\\n1. **Job Market**: AI has already started replacing certain jobs, particularly those that involve repetitive tasks or data processing. While this might seem daunting, it's also creating new job opportunities in fields like AI development, deployment, and maintenance.\\n2. **Healthcare**: AI-powered diagnostic tools are revolutionizing healthcare by improving diagnosis accuracy, reducing costs, and enabling early detection of diseases. Additionally, AI-assisted robots are helping with surgeries and patient care.\\n3. **Education**: AI is enhancing the learning experience through personalized learning platforms, adaptive assessments, and intelligent tutoring systems. This has the potential to bridge the education gap for millions of students worldwide.\\n4. **Transportation**: Self-driving cars, drones, and autonomous trucks are transforming the way we travel and transport goods. AI-powered traffic management systems are also reducing congestion and improving safety.\\n5. **Economy**: AI is increasing productivity, efficiency, and competitiveness in various industries, such as finance, retail, and manufacturing. It's also helping to create new business opportunities and job growth.\\n6. **Cybersecurity**: As AI becomes more pervasive, cybersecurity threats are escalating. AI-powered systems can help detect and prevent cyberattacks, ensuring a safer online environment for individuals and organizations.\\n7. **Environmental Sustainability**: AI is being used to monitor and mitigate the effects of climate change, optimize energy consumption, and develop sustainable solutions for waste management and conservation.\\n\\nWhile AI has many benefits, it's essential to acknowledge the potential risks and challenges associated with its development and deployment. As we continue to integrate AI into our society, it's crucial that we prioritize ethical considerations, such as data privacy, transparency, and accountability.\\n\\nWhat specific aspects of AI's impact on society would you like me to explore further?\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,40,2846,264,3544,4221,1646,11,358,1541,956,617,4443,19459,477,279,5845,311,19635,3230,21633,477,13650,13,9062,892,499,16681,449,757,11,433,596,264,502,10652,323,358,1212,505,19307,13,4452,11,358,649,1456,311,25,220,16,13,5560,2317,25,1442,584,2351,25394,264,8712,430,45395,5361,6743,11,358,649,1005,279,2317,315,1057,3766,6743,311,6179,856,14847,13,220,17,13,29734,1203,311,3766,6743,25,1442,499,21650,5905,264,3766,1984,477,8712,11,358,649,1427,709,279,9959,2038,323,6013,28178,13,220,18,13,40665,4689,2038,25,1442,499,2610,922,264,8712,584,3077,14407,1603,11,358,649,3493,4689,2038,477,26793,5552,311,430,8712,13,3011,1694,1071,11,358,1541,956,617,279,5845,311,19635,3230,21633,477,13650,505,3766,22639,13,9062,16628,449,757,374,264,502,1212,0,83017,15592,596,5536,389,8396,13,128009,128006,78191,128007,271,3923,264,2294,8712,0,1666,264,3544,4221,1646,11,358,3358,656,856,1888,311,3493,4689,26793,323,2038,922,15592,596,5536,389,8396,382,9470,16895,11478,320,15836,8,706,1027,46890,279,1648,584,3974,11,990,11,323,16681,449,1855,1023,520,459,31069,18338,13,5810,527,1063,1401,5789,1405,15592,374,3339,264,5199,5536,1473,16,13,3146,12524,8152,96618,15592,706,2736,3940,25935,3738,7032,11,8104,1884,430,21736,59177,9256,477,828,8863,13,6104,420,2643,2873,57697,11,433,596,1101,6968,502,2683,10708,304,5151,1093,15592,4500,11,24047,11,323,13709,627,17,13,3146,14884,10727,96618,15592,41503,15439,7526,527,14110,4954,18985,555,18899,23842,13708,11,18189,7194,11,323,28462,4216,18468,315,19338,13,23212,11,15592,35915,13539,29807,527,10695,449,68823,323,8893,2512,627,18,13,3146,40234,96618,15592,374,47594,279,6975,3217,1555,35649,6975,15771,11,48232,41300,11,323,25530,78143,6067,13,1115,706,279,4754,311,14497,279,6873,13225,369,11990,315,4236,15603,627,19,13,3146,28660,367,96618,10323,60811,9515,11,38332,11,323,39293,27861,527,46890,279,1648,584,5944,323,7710,11822,13,15592,41503,9629,6373,6067,527,1101,18189,55146,323,18899,7296,627,20,13,3146,36,71171,96618,15592,374,7859,26206,11,15374,11,323,77651,304,5370,19647,11,1778,439,17452,11,11040,11,323,15266,13,1102,596,1101,10695,311,1893,502,2626,10708,323,2683,6650,627,21,13,3146,57815,655,17476,96618,1666,15592,9221,810,71867,11,62542,18208,527,77157,13,15592,41503,6067,649,1520,11388,323,5471,21516,81210,11,23391,264,30549,2930,4676,369,7931,323,11351,627,22,13,3146,83166,89812,96618,15592,374,1694,1511,311,8891,323,50460,279,6372,315,10182,2349,11,30536,4907,15652,11,323,2274,22556,10105,369,12571,6373,323,29711,382,8142,15592,706,1690,7720,11,433,596,7718,311,25670,279,4754,15635,323,11774,5938,449,1202,4500,323,24047,13,1666,584,3136,311,32172,15592,1139,1057,8396,11,433,596,16996,430,584,63652,31308,38864,11,1778,439,828,12625,11,28330,11,323,39242,382,3923,3230,13878,315,15592,596,5536,389,8396,1053,499,1093,757,311,13488,4726,30],\"total_duration\":47182820917,\"load_duration\":830583958,\"prompt_eval_count\":185,\"prompt_eval_duration\":6191000000,\"eval_count\":420,\"eval_duration\":40153000000}\n",
            "üìä Initial Compression Ratio: 6.00\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 48.0699 sec | üì¶ Compressed KV Cache Size: 451 bytes\n",
            "üñ•Ô∏è Memory Usage Change: 0.0 MB\n",
            "\n",
            "Response 1: {'response': \"What a great topic! As a large language model, I'll do my best to provide general insights and information about AI's impact on society.\\n\\nArtificial intelligence (AI) has been transforming the way we live, work, and interact with each other at an unprecedented pace. Here are some key areas where AI is making a significant impact:\\n\\n1. **Job Market**: AI has already started replacing certain jobs, particularly those that involve repetitive tasks or data processing. While this might seem daunting, it's also creating new job opportunities in fields like AI development, deployment, and maintenance.\\n2. **Healthcare**: AI-powered diagnostic tools are revolutionizing healthcare by improving diagnosis accuracy, reducing costs, and enabling early detection of diseases. Additionally, AI-assisted robots are helping with surgeries and patient care.\\n3. **Education**: AI is enhancing the learning experience through personalized learning platforms, adaptive assessments, and intelligent tutoring systems. This has the potential to bridge the education gap for millions of students worldwide.\\n4. **Transportation**: Self-driving cars, drones, and autonomous trucks are transforming the way we travel and transport goods. AI-powered traffic management systems are also reducing congestion and improving safety.\\n5. **Economy**: AI is increasing productivity, efficiency, and competitiveness in various industries, such as finance, retail, and manufacturing. It's also helping to create new business opportunities and job growth.\\n6. **Cybersecurity**: As AI becomes more pervasive, cybersecurity threats are escalating. AI-powered systems can help detect and prevent cyberattacks, ensuring a safer online environment for individuals and organizations.\\n7. **Environmental Sustainability**: AI is being used to monitor and mitigate the effects of climate change, optimize energy consumption, and develop sustainable solutions for waste management and conservation.\\n\\nWhile AI has many benefits, it's essential to acknowledge the potential risks and challenges associated with its development and deployment. As we continue to integrate AI into our society, it's crucial that we prioritize ethical considerations, such as data privacy, transparency, and accountability.\\n\\nWhat specific aspects of AI's impact on society would you like me to explore further?\", 'inference_time': 48.0699, 'compressed_cache_size': 451, 'memory_change': 0.0}\n",
            "\n",
            "üü¢ Running Inference 2\n",
            "\n",
            "üì§ Sending Request to API: https://71cd-91-104-75-197.ngrok-free.app/api/generate\n",
            "üìú Payload: {\n",
            "  \"model\": \"llama3\",\n",
            "  \"prompt\": \"tasks or data processing. While this might seem daunting, it's also creating new job opportunities in fields like AI development, deployment, and maintenance. 2. **Healthcare**: AI-powered diagnostic tools are revolutionizing healthcare by improving diagnosis accuracy, reducing costs, and enabling early detection of diseases. Additionally, AI-assisted robots are helping with surgeries and patient care. 3. **Education**: AI is enhancing the learning experience through personalized learning platforms, adaptive assessments, and intelligent tutoring systems. This has the potential to bridge the education gap for millions of students worldwide. 4. **Transportation**: Self-driving cars, drones, and autonomous trucks are transforming the way we travel and transport goods. AI-powered traffic management systems are also reducing congestion and improving safety. 5. **Economy**: AI is increasing productivity, efficiency, and competitiveness in various industries, such as finance, retail, and manufacturing. It's also helping to create new business opportunities and job growth. 6. **Cybersecurity**: As AI becomes more pervasive, cybersecurity threats are escalating. AI-powered systems can help detect and prevent cyberattacks, ensuring a safer online environment for individuals and organizations. 7. **Environmental Sustainability**: AI is being used to monitor and mitigate the effects of climate change, optimize energy consumption, and develop sustainable solutions for waste management and conservation. While AI has many benefits, it's essential to acknowledge the potential risks and challenges associated with its development and deployment. As we continue to integrate AI into our society, it's crucial that we prioritize ethical considerations, such as data privacy, transparency, and accountability. What specific aspects of AI's impact on society would you like me to explore further? How does reinforcement learning work?\",\n",
            "  \"stream\": false\n",
            "}\n",
            "üì© API Response Status: 200\n",
            "üì© API Response Text: {\"model\":\"llama3\",\"created_at\":\"2025-02-17T01:19:26.10267Z\",\"response\":\"I'm happy to dive deeper into the various aspects of AI's impact on society.\\n\\nRegarding your question about how reinforcement learning works:\\n\\nReinforcement learning is a type of machine learning that involves training an agent (such as a robot or AI system) to make decisions in complex, uncertain environments. The goal is to maximize a reward signal while interacting with the environment.\\n\\nHere's a high-level overview of the process:\\n\\n1. **Agent-Environment Interaction**: The agent takes actions in the environment, which causes changes in the state of the environment.\\n2. **Reward Calculation**: The agent receives a reward or penalty based on its actions and the current state of the environment.\\n3. **Value Update**: The agent updates its value function (a measure of the expected return) using the received reward and the updated state of the environment.\\n4. **Policy Update**: The agent updates its policy (the probability of taking each possible action in a given state) based on the updated value function.\\n5. **Repeat**: Steps 1-4 are repeated, with the agent continually exploring the environment, learning from rewards, and refining its policy.\\n\\nThis process can take millions or even billions of iterations to converge, but it allows agents to learn complex behaviors and decision-making strategies without explicitly programming them.\\n\\nWould you like me to explore any specific aspects of AI's impact on society, such as:\\n\\n* The role of AI in job creation and displacement\\n* AI-powered healthcare innovations\\n* The ethics of AI development and deployment\\n* AI applications in education and learning\\n\\nLet me know, and I'll be happy to provide more insights!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,25792,477,828,8863,13,6104,420,2643,2873,57697,11,433,596,1101,6968,502,2683,10708,304,5151,1093,15592,4500,11,24047,11,323,13709,13,220,17,13,3146,14884,10727,96618,15592,41503,15439,7526,527,14110,4954,18985,555,18899,23842,13708,11,18189,7194,11,323,28462,4216,18468,315,19338,13,23212,11,15592,35915,13539,29807,527,10695,449,68823,323,8893,2512,13,220,18,13,3146,40234,96618,15592,374,47594,279,6975,3217,1555,35649,6975,15771,11,48232,41300,11,323,25530,78143,6067,13,1115,706,279,4754,311,14497,279,6873,13225,369,11990,315,4236,15603,13,220,19,13,3146,28660,367,96618,10323,60811,9515,11,38332,11,323,39293,27861,527,46890,279,1648,584,5944,323,7710,11822,13,15592,41503,9629,6373,6067,527,1101,18189,55146,323,18899,7296,13,220,20,13,3146,36,71171,96618,15592,374,7859,26206,11,15374,11,323,77651,304,5370,19647,11,1778,439,17452,11,11040,11,323,15266,13,1102,596,1101,10695,311,1893,502,2626,10708,323,2683,6650,13,220,21,13,3146,57815,655,17476,96618,1666,15592,9221,810,71867,11,62542,18208,527,77157,13,15592,41503,6067,649,1520,11388,323,5471,21516,81210,11,23391,264,30549,2930,4676,369,7931,323,11351,13,220,22,13,3146,83166,89812,96618,15592,374,1694,1511,311,8891,323,50460,279,6372,315,10182,2349,11,30536,4907,15652,11,323,2274,22556,10105,369,12571,6373,323,29711,13,6104,15592,706,1690,7720,11,433,596,7718,311,25670,279,4754,15635,323,11774,5938,449,1202,4500,323,24047,13,1666,584,3136,311,32172,15592,1139,1057,8396,11,433,596,16996,430,584,63652,31308,38864,11,1778,439,828,12625,11,28330,11,323,39242,13,3639,3230,13878,315,15592,596,5536,389,8396,1053,499,1093,757,311,13488,4726,30,2650,1587,72378,6975,990,30,128009,128006,78191,128007,271,40,2846,6380,311,30963,19662,1139,279,5370,13878,315,15592,596,5536,389,8396,382,85625,701,3488,922,1268,72378,6975,4375,1473,697,258,11167,6975,374,264,955,315,5780,6975,430,18065,4967,459,8479,320,21470,439,264,12585,477,15592,1887,8,311,1304,11429,304,6485,11,36218,22484,13,578,5915,374,311,35608,264,11565,8450,1418,45830,449,279,4676,382,8586,596,264,1579,11852,24131,315,279,1920,1473,16,13,3146,17230,12,13013,43807,96618,578,8479,5097,6299,304,279,4676,11,902,11384,4442,304,279,1614,315,279,4676,627,17,13,3146,60722,75316,96618,578,8479,21879,264,11565,477,16750,3196,389,1202,6299,323,279,1510,1614,315,279,4676,627,18,13,3146,1150,5666,96618,578,8479,9013,1202,907,734,320,64,6767,315,279,3685,471,8,1701,279,4036,11565,323,279,6177,1614,315,279,4676,627,19,13,3146,14145,5666,96618,578,8479,9013,1202,4947,320,1820,19463,315,4737,1855,3284,1957,304,264,2728,1614,8,3196,389,279,6177,907,734,627,20,13,3146,39818,96618,40961,220,16,12,19,527,11763,11,449,279,8479,35611,24919,279,4676,11,6975,505,21845,11,323,74285,1202,4947,382,2028,1920,649,1935,11990,477,1524,33151,315,26771,311,80867,11,719,433,6276,13307,311,4048,6485,28198,323,5597,28846,15174,2085,21650,15840,1124,382,29089,499,1093,757,311,13488,904,3230,13878,315,15592,596,5536,389,8396,11,1778,439,1473,9,578,3560,315,15592,304,2683,9886,323,44153,198,9,15592,41503,18985,46045,198,9,578,32008,315,15592,4500,323,24047,198,9,15592,8522,304,6873,323,6975,271,10267,757,1440,11,323,358,3358,387,6380,311,3493,810,26793,0],\"total_duration\":34483821458,\"load_duration\":72324333,\"prompt_eval_count\":354,\"prompt_eval_duration\":3327000000,\"eval_count\":325,\"eval_duration\":31069000000}\n",
            "üìä Compression Ratio: 5.59 | üîÑ Change: -6.94%\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 35.257 sec | üì¶ Compressed KV Cache Size: 736 bytes\n",
            "üñ•Ô∏è Memory Usage Change: 0.0 MB\n",
            "\n",
            "Response 2: {'response': \"I'm happy to dive deeper into the various aspects of AI's impact on society.\\n\\nRegarding your question about how reinforcement learning works:\\n\\nReinforcement learning is a type of machine learning that involves training an agent (such as a robot or AI system) to make decisions in complex, uncertain environments. The goal is to maximize a reward signal while interacting with the environment.\\n\\nHere's a high-level overview of the process:\\n\\n1. **Agent-Environment Interaction**: The agent takes actions in the environment, which causes changes in the state of the environment.\\n2. **Reward Calculation**: The agent receives a reward or penalty based on its actions and the current state of the environment.\\n3. **Value Update**: The agent updates its value function (a measure of the expected return) using the received reward and the updated state of the environment.\\n4. **Policy Update**: The agent updates its policy (the probability of taking each possible action in a given state) based on the updated value function.\\n5. **Repeat**: Steps 1-4 are repeated, with the agent continually exploring the environment, learning from rewards, and refining its policy.\\n\\nThis process can take millions or even billions of iterations to converge, but it allows agents to learn complex behaviors and decision-making strategies without explicitly programming them.\\n\\nWould you like me to explore any specific aspects of AI's impact on society, such as:\\n\\n* The role of AI in job creation and displacement\\n* AI-powered healthcare innovations\\n* The ethics of AI development and deployment\\n* AI applications in education and learning\\n\\nLet me know, and I'll be happy to provide more insights!\", 'inference_time': 35.257, 'compressed_cache_size': 736, 'memory_change': 0.0}\n",
            "üóëÔ∏è KV Cache Cleared.\n",
            "\n",
            "üü¢ Running Inference 3 (After Cache Clear)\n",
            "\n",
            "üì§ Sending Request to API: https://71cd-91-104-75-197.ngrok-free.app/api/generate\n",
            "üìú Payload: {\n",
            "  \"model\": \"llama3\",\n",
            "  \"prompt\": \"Can you recall previous topics?\",\n",
            "  \"stream\": false\n",
            "}\n",
            "üì© API Response Status: 200\n",
            "üì© API Response Text: {\"model\":\"llama3\",\"created_at\":\"2025-02-17T01:19:46.928221Z\",\"response\":\"I'm a large language model, I don't have personal memories or the ability to recall specific conversations or topics. Each time you interact with me, it's a new conversation and I don't retain any information from previous chats.\\n\\nHowever, I can try to:\\n\\n1. Use contextual clues: If we're discussing a topic that's related to a previous conversation, I might be able to pick up on some common themes or keywords and use them to inform my response.\\n2. Draw from general knowledge: As a knowledgeable AI model, I have been trained on vast amounts of text data and can draw upon this knowledge base to provide information on various topics, including those we may have discussed previously.\\n3. Use conversation history: Some platforms or services that integrate with me might keep track of our conversation history, allowing me to access previous topics or context.\\n\\nKeep in mind that my recall is limited to the scope of my training data and the specific features implemented by the platform or service. If you'd like to discuss a specific topic we previously covered, feel free to bring it up again, and I'll do my best to provide helpful information!\",\"done\":true,\"done_reason\":\"stop\",\"context\":[128006,882,128007,271,6854,499,19635,3766,13650,30,128009,128006,78191,128007,271,40,2846,264,3544,4221,1646,11,358,1541,956,617,4443,19459,477,279,5845,311,19635,3230,21633,477,13650,13,9062,892,499,16681,449,757,11,433,596,264,502,10652,323,358,1541,956,14389,904,2038,505,3766,49626,382,11458,11,358,649,1456,311,1473,16,13,5560,66251,43775,25,1442,584,2351,25394,264,8712,430,596,5552,311,264,3766,10652,11,358,2643,387,3025,311,3820,709,389,1063,4279,22100,477,21513,323,1005,1124,311,6179,856,2077,627,17,13,12261,505,4689,6677,25,1666,264,42066,15592,1646,11,358,617,1027,16572,389,13057,15055,315,1495,828,323,649,4128,5304,420,6677,2385,311,3493,2038,389,5370,13650,11,2737,1884,584,1253,617,14407,8767,627,18,13,5560,10652,3925,25,4427,15771,477,3600,430,32172,449,757,2643,2567,3839,315,1057,10652,3925,11,10923,757,311,2680,3766,13650,477,2317,382,19999,304,4059,430,856,19635,374,7347,311,279,7036,315,856,4967,828,323,279,3230,4519,11798,555,279,5452,477,2532,13,1442,499,4265,1093,311,4358,264,3230,8712,584,8767,9960,11,2733,1949,311,4546,433,709,1578,11,323,358,3358,656,856,1888,311,3493,11190,2038,0],\"total_duration\":20082239666,\"load_duration\":50409750,\"prompt_eval_count\":16,\"prompt_eval_duration\":324000000,\"eval_count\":229,\"eval_duration\":19698000000}\n",
            "üìä Compression Ratio: 5.66 | üîÑ Change: 1.34%\n",
            "üíæ KV Cache (Compressed) saved to file.\n",
            "‚è≥ Inference Time: 20.5502 sec | üì¶ Compressed KV Cache Size: 168 bytes\n",
            "üñ•Ô∏è Memory Usage Change: 0.0 MB\n",
            "\n",
            "Response 3: {'response': \"I'm a large language model, I don't have personal memories or the ability to recall specific conversations or topics. Each time you interact with me, it's a new conversation and I don't retain any information from previous chats.\\n\\nHowever, I can try to:\\n\\n1. Use contextual clues: If we're discussing a topic that's related to a previous conversation, I might be able to pick up on some common themes or keywords and use them to inform my response.\\n2. Draw from general knowledge: As a knowledgeable AI model, I have been trained on vast amounts of text data and can draw upon this knowledge base to provide information on various topics, including those we may have discussed previously.\\n3. Use conversation history: Some platforms or services that integrate with me might keep track of our conversation history, allowing me to access previous topics or context.\\n\\nKeep in mind that my recall is limited to the scope of my training data and the specific features implemented by the platform or service. If you'd like to discuss a specific topic we previously covered, feel free to bring it up again, and I'll do my best to provide helpful information!\", 'inference_time': 20.5502, 'compressed_cache_size': 168, 'memory_change': 0.0}\n"
          ]
        }
      ]
    }
  ]
}